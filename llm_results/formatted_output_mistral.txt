Context: The Digital Personal Data Protection Bill, 2023 was introduced in Lok Sabha on August 3, 2023. The Bill seeks to provide for the protection of personal data and the privacy of individuals. Applicability: The Bill applies to the processing of digital personal data within India where such data is: (i) collected online, or (ii) collected offline and is digitised. It will also apply to the processing of personal data outside India if it is for offering goods or services in India. Personal data is defined as any data about an individual who is identifiable by or in relation to such data. Processing has been defined as wholly or partially automated operation or set of operations performed on digital personal data. It includes collection, storage, use, and sharing. Consent: Personal data may be processed only for a lawful purpose after obtaining the consent of the individual. A notice must be given before seeking consent. The notice should contain details about the personal data to be collected and the purpose of processing. Consent may be withdrawn at any point in time. Consent will not be required for 'legitimate uses' including: (i) specified purpose for which data has been provided by an individual voluntarily, (ii) provision of benefit or service by the government, (iii) medical emergency, and (iv) employment. For individuals below 18 years of age, consent will be provided by the parent or the legal guardian. Rights and duties of data principal: An individual, whose data is being processed (data principal), will have the right to: (i) obtain information about processing, (ii) seek correction and erasure of personal data, (iii) nominate another person to exercise rights in the event of death or incapacity, and (iv) grievance redressal. Data principals will have certain duties. They must not: (i) register a false or frivolous complaint, and (ii) furnish any false particulars or impersonate another person in specified cases. Violation of duties will be punishable with a penalty of up to Rs 10,000. Obligations of data fiduciaries: The entity, determining the purpose and means of processing, (data fiduciary), must: (i) make reasonable efforts to ensure the accuracy and completeness of data, (ii) build reasonable security safeguards to prevent a data breach, (iii) inform the Data Protection Board of India and affected persons in the event of a breach, and (iv) erase personal data as soon as the purpose has been met and retention is not necessary for legal purposes (storage limitation). In case of government entities, storage limitation and the right of the data principal to erasure will not apply. Significant data fiduciaries: Certain data fiduciaries may be designated as significant data fiduciaries. Certain factors must be taken into regard such as: (i) volume and sensitivity of personal data processed, (ii) risks to the rights of data principals, (iii) security of the state, and (iv) public order. These entities will have certain additional obligations including: (i) appointing a data protection officer, and (ii) undertaking impact assessment and compliance audit. Exemptions: Rights of the data principal and obligations of data fiduciaries (except data security) will not apply in specified cases. These include: (i) prevention and investigation of offences, and (ii) enforcement of legal rights or claims. The central government may, by notification, exempt certain activities from the application of the Bill. These include: (i) processing by government entities in the interest of the security of the state and public order, and (ii) research, archiving, or statistical purposes. Processing of personal data of children: While processing the personal data of a child, the data fiduciary must not undertake: (i) processing that is likely to cause any detrimental effect on the well-being of the child, and (ii) tracking, behavioural monitoring, or targeted advertising. Cross-border transfer: The Bill allows the transfer of personal data outside India, except to countries restricted by the government through notification. Data Protection Board of India: The central government will establish the Data Protection Board of India. Key functions of the Board include: (i) monitoring compliance and imposing penalties, (ii) directing data fiduciaries to take necessary measures in the event of a data breach, and (iii) hearing grievances made by affected persons. Board members will be appointed for two years and will be eligible for re-appointment. Penalties: The schedule to the Bill specifies penalties for various offences such as up to: (i) Rs 200 crore for non-fulfilment of obligations for children, and (ii) Rs 250 crore for failure to take security measures to prevent data breaches.

**Question:** What are the key applicability criteria for the Digital Personal Data Protection Bill, 2023?

**LLM Answer:**  The key applicability criteria for the Digital Personal Data Protection Bill, 2023 are as follows:

1. The processing of digital personal data within India where such data is either collected online or collected offline and subsequently digitized.
2. The processing of personal data outside India if it is for offering goods or services in India.

In other words, the bill applies to both domestic and international processing activities involving personal data, as long as they meet one of the two criteria mentioned above.

**Human Answer:** Applicability Criteria: The Bill applies to the processing of digital personal data within India if the data is (i) collected online, or (ii) collected offline and then digitized. It also applies to processing personal data outside India if it involves offering goods or services within India.

**Scores:** Similarity: 0.904, ROUGE-L: 0.512, BLEU: 0.222, METEOR: 0.578, BERT Precision: 0.895, BERT Recall: 0.937, BERT F1: 0.915

**Question:** Under what conditions can personal data be processed without the consent of the individual?

**LLM Answer:**  Personal data may be processed without the consent of the individual in the following conditions:
1. For 'legitimate uses' including:
   - Specified purpose for which data has been provided by an individual voluntarily
   - Provision of benefit or service by the government
   - Medical emergency
   - Employment

2. For individuals below 18 years of age, consent will be provided by the parent or legal guardian.
3. In specified cases exempted from the application of the Bill such as prevention and investigation of offences, enforcement of legal rights or claims. The central government may also exempt certain activities from the application of the Bill.

**Human Answer:** Conditions for Processing Data Without Consent: Personal data can be processed without individual consent for 'legitimate uses,' including when data is voluntarily provided for a specified purpose, for the provision of government benefits or services, during medical emergencies, and for employment purposes.

**Scores:** Similarity: 0.885, ROUGE-L: 0.298, BLEU: 0.025, METEOR: 0.453, BERT Precision: 0.84, BERT Recall: 0.907, BERT F1: 0.872

**Question:** What rights are granted to data principals under the Bill?

**LLM Answer:**  The Digital Personal Data Protection Bill, as described in your context, grants the following rights to data principals:

1. Right to obtain information about processing: A data principal has the right to know how their personal data is being processed.
2. Seek correction and erasure of personal data: A data principal can request for the correction or deletion of inaccurate or unnecessary personal data.
3. Nominate another person to exercise rights in the event of death or incapacity: A data principal can appoint a representative to manage their data-related matters when they are unable to do so.
4. Grievance redressal: A data principal has the right to seek resolution for any issues related to the processing of their personal data.

**Human Answer:** Rights of Data Principals: Data principals, or individuals whose data is being processed, have the right to (i) obtain information about the processing of their data, (ii) seek correction and erasure of their personal data, (iii) nominate someone to exercise their rights if they die or become incapacitated, and (iv) grievance redressal.

**Scores:** Similarity: 0.825, ROUGE-L: 0.289, BLEU: 0.116, METEOR: 0.441, BERT Precision: 0.874, BERT Recall: 0.886, BERT F1: 0.88

**Question:** What obligations do data fiduciaries have in ensuring data protection and security?

**LLM Answer:**  Data fiduciaries, being the entities that determine the purpose and means of processing personal data, have several obligations in ensuring data protection and security. These include:

1. Making reasonable efforts to ensure the accuracy and completeness of the data.
2. Building reasonable security safeguards to prevent a data breach.
3. Informing the Data Protection Board of India and affected persons in the event of a breach.
4. Erasing personal data as soon as the purpose has been met and retention is not necessary for legal purposes (storage limitation). However, this does not apply in case of government entities.
5. Appointing a data protection officer and undertaking impact assessment and compliance audit if designated as significant data fiduciaries.

**Human Answer:** Obligations of Data Fiduciaries: Data fiduciaries, entities determining the purpose and means of data processing, must (i) make reasonable efforts to ensure data accuracy and completeness, (ii) build reasonable security safeguards to prevent data breaches, (iii) inform the Data Protection Board of India and affected individuals of data breaches, and (iv) erase personal data when its purpose has been met and retention is no longer legally necessary. Note that storage limitation and the right of the data principal to erasure do not apply to government entities.

**Scores:** Similarity: 0.907, ROUGE-L: 0.532, BLEU: 0.233, METEOR: 0.608, BERT Precision: 0.906, BERT Recall: 0.913, BERT F1: 0.909

**Question:** What are the penalties specified in the Bill for non-compliance with data protection obligations?

**LLM Answer:**  The Digital Personal Data Protection Bill, 2023 specifies penalties for various offences up to:

  1. Rs 200 crore for non-fulfilment of obligations related to children's personal data protection.

  2. Rs 250 crore for failure to take security measures to prevent data breaches.

**Human Answer:** Penalties for Non-Compliance: The Bill specifies penalties for various offenses, including (i) up to Rs 200 crore for not fulfilling obligations related to children's data, and (ii) up to Rs 250 crore for failing to implement security measures to prevent data breaches

**Scores:** Similarity: 0.816, ROUGE-L: 0.682, BLEU: 0.324, METEOR: 0.588, BERT Precision: 0.907, BERT Recall: 0.916, BERT F1: 0.912


--------------------------------------------------------------------------------

Context: The committee recommends the following: 1. To implement a whole-of-government approach to AI Governance, MeitY and the Principal Scientific Adviser should establish an empowered mechanism to coordinate AI Governance. The empowered mechanism should be in the form of an Inter-Ministerial AI Coordination Committee or Governance Group (Committee/ Group). It should bring together the various authorities and institutions that deal with AI Governance at the national level. The Committee/ Group should have an ongoing status and should not be a limited duration mechanism. The overall purpose of this Committee/ Group should be to bring the key institutions around a common roadmap and to coordinate their efforts to implement a whole-of government approach. A collaborative and co-ordinated approach by various regulators can enable them to be more efficient and effective, given the complexity likely to be involved in dealing with AI systems at scale, especially when we take a long-term view of the diffusion stage of their lifecycle. This can be especially necessary in domains and areas where multiple authorities may be concerned (e.g., consumer protection, food, transportation, agriculture, health care, etc.). The Committee/ Group should enable a whole of government approach to the AI ecosystem. Currently, regulators and government departments may have some visibility on the AI systems developed or deployed by entities who are under sectoral regulation (e.g., finance or health) or where the market is concentrated (e.g., ecommerce, social media, aggregators). However, the level of visibility would need to be adequate to assess potential risks associated with such entities in the context of AI. Further, there are likely to be AI systems developed or being developed and/ or deployed by entities who may not have an interface with the government/ regulators from a perspective of affording suitable visibility to enable a risk assessment in relation to AI. A pre-requisite of governance would be for the government and the regulators to have a credible understanding the AI ecosystem in the country so that governance measures are rooted to the realities of existing and likely risks. The Committee/ Group should facilitate this task. This would require a conversation-led approach with a view to develop an understanding of the ecosystem which can both serve as feedback for strengthening governance and enable understanding of possible challenges and gaps in complying Page 13 of 22 and enforcing existing laws. It is important to emphasise that such a mapping exercise should not result in regulatory overreach through at scale registration and reporting requirements. With the above context, the Committee/ Group should meet at a regular basis to suggest measures to catalyse collaboration between departments and regulators, so that they can: • apply and strengthen existing laws to minimise risk of harm due to use of AI; • provide legal clarity and certainty around development and use of AI by issuing joint guidance; • harmonise existing efforts and initiatives around common terminologies and risk inventories; • enable demonstrable self-regulation to operationalise the responsible AI principles; • take coordinated steps to respond to identified gaps with the benefit of multiregulatory support; • create a policy environment which enables the use of AI for beneficial use-cases; and • promote the development and deployment of responsible AI applications in their domains/sectors. In order to enable appropriate measurement of fairness, accountability and transparency in the Indian context, it is an essential prerequisite to have access to the right datasets, relevant to the Indian context, which allows users to assess the fairness and bias of their models across standard datasets. The creation of better datasets for the Indian context should be stimulated, and sector-specific datasets should be identified to enable creation and evaluation of fair models. These initiatives may be encouraged by the Committee/ Group. The Committee/ Group should have a mix of both official and non-official members, because such a forum focused on coordinating AI governance must also bring in external expertise from industry and academia, given their central role in operationalising responsible AI principles in practice. It may be headed by the Principal Scientific Adviser. Official members could include representatives deputed from MeitY, the NITI Aayog, the Telecommunication Engineering Centre, Bureau of Indian Standards, other departments of the Central Government, as well as sectoral regulators (e.g., RBI, Indian Council of Medical Research, SEBI, IRDAI, Telecom Regulatory Authority of India, etc.). Non-official members could include persons capable of representing the interests of AI developers, AI deployers, data providers, data principals, and end-users – so that the perspectives of the overall ecosystem can be considered. The Committee/ Group should invite external experts for discussions to understand and take on board diverse perspectives.

**Question:** What is the primary objective of the proposed Inter-Ministerial AI Coordination Committee or Governance Group?

**LLM Answer:**  The primary objective of the proposed Inter-Ministerial AI Coordination Committee or Governance Group is to bring key institutions around a common roadmap and coordinate their efforts to implement a whole-of-government approach, with an emphasis on minimizing risk of harm due to the use of AI, providing legal clarity and certainty around development and use of AI, harmonising existing efforts and initiatives, enabling demonstrable self-regulation, taking coordinated steps to respond to identified gaps, creating a policy environment which enables the use of AI for beneficial use-cases, promoting the development and deployment of responsible AI applications, stimulating the creation of better datasets for the Indian context, and inviting external experts for discussions to understand and take on board diverse perspectives.

**Human Answer:** Primary Objective: The primary objective of the proposed Inter-Ministerial AI Coordination Committee or Governance Group is to bring key institutions together around a common roadmap and to coordinate their efforts to implement a whole-of-government approach to AI governance1. This includes enabling a whole-of-government approach to the AI ecosystem.

**Scores:** Similarity: 0.859, ROUGE-L: 0.466, BLEU: 0.223, METEOR: 0.588, BERT Precision: 0.876, BERT Recall: 0.947, BERT F1: 0.91

**Question:** Why is a whole-of-government approach considered essential for AI governance in India?

**LLM Answer:**  A whole-of-government approach is considered essential for AI governance in India because the complexity involved in dealing with AI systems at scale, especially when taking a long-term view of their diffusion stage, necessitates coordination between various authorities and institutions. This is particularly important in domains where multiple authorities may be concerned (e.g., consumer protection, food, transportation, agriculture, health care, etc.). The aim is to bring these key institutions around a common roadmap, apply and strengthen existing laws to minimize risk of harm due to AI use, provide legal clarity and certainty, harmonize efforts and initiatives, enable demonstrable self-regulation, take coordinated steps to respond to identified gaps, create a policy environment that enables the beneficial use of AI, promote responsible AI applications in their domains/sectors, and measure fairness, accountability, and transparency in the Indian context by having access to right datasets relevant to the Indian context. The approach also ensures that external expertise from industry and academia, who play a central role in operationalizing responsible AI principles in practice, are involved in the decision-making process.

**Human Answer:** Importance of a Whole-of-Government Approach: A whole-of-government approach is considered essential because a collaborative and coordinated approach by various regulators can enable them to be more efficient and effective, given the complexity likely to be involved in dealing with AI systems at scale, especially in the long term1. This is particularly necessary in areas where multiple authorities may be concerned, such as consumer protection, food, transportation, agriculture, and health care.

**Scores:** Similarity: 0.671, ROUGE-L: 0.298, BLEU: 0.159, METEOR: 0.53, BERT Precision: 0.864, BERT Recall: 0.925, BERT F1: 0.893

**Question:** What are the key functions and responsibilities of the AI Governance Committee/Group?

**LLM Answer:**  The key functions and responsibilities of the AI Governance Committee/Group, as outlined in the provided context, include:

1. Coordinating efforts to implement a whole-of-government approach to AI Governance by bringing together various authorities and institutions that deal with AI Governance at the national level.
2. Establishing a common roadmap for the key institutions involved in AI Governance.
3. Facilitating an understanding of the AI ecosystem in the country, including mapping out the various entities developing or deploying AI systems, both those with and without government/regulator interface.
4. Suggesting measures to catalyze collaboration between departments and regulators so they can:
   - Apply and strengthen existing laws to minimize risks associated with AI use.
   - Provide legal clarity and certainty around the development and use of AI.
   - Harmonize efforts and initiatives around common terminologies and risk inventories.
   - Enable demonstrable self-regulation to operationalize responsible AI principles.
   - Take coordinated steps to respond to identified gaps with the benefit of multiregulatory support.
   - Create a policy environment that encourages the use of AI for beneficial use cases.
   - Promote the development and deployment of responsible AI applications in their respective domains/sectors.
5. Encouraging initiatives to stimulate the creation of better datasets for the Indian context, including sector-specific datasets for creating and evaluating fair models.
6. Having a mix of official and non-official members, with external expertise from industry and academia to operationalize responsible AI principles in practice. This includes representatives from MeitY, NITI Aayog, Telecommunication Engineering Centre, Bureau of Indian Standards, other departments of the Central Government, sectoral regulators, as well as AI developers, deployers, data providers, data principals, and end-users.
7. Inviting external experts for discussions to understand and take on board diverse perspectives in order to gain a comprehensive understanding of the AI ecosystem.

**Human Answer:** Key Functions and Responsibilities: The AI Governance Committee/Group is responsible for: 1) Suggesting measures to catalyze collaboration between departments and regulators. 2) Applying and strengthening existing laws to minimize the risk of harm due to the use of AI. 3) Providing legal clarity and certainty around the development and use of AI by issuing joint guidance.4) Harmonizing existing efforts and initiatives around common terminologies and risk inventories. 5) Enabling demonstrable self-regulation to operationalize responsible AI principles. 6) Taking coordinated steps to respond to identified gaps with multi-regulatory support. 7) Creating a policy environment that enables the use of AI for beneficial use-cases. 8) Promoting the development and deployment of responsible AI applications in various domains and sectors.

**Scores:** Similarity: 0.884, ROUGE-L: 0.455, BLEU: 0.198, METEOR: 0.48, BERT Precision: 0.849, BERT Recall: 0.923, BERT F1: 0.885

**Question:** How does the committee propose to ensure fairness, accountability, and transparency in AI systems within the Indian context?

**LLM Answer:**  The committee proposes to ensure fairness, accountability, and transparency in AI systems within the Indian context by suggesting the following measures:

1. **Access to right datasets**: The creation of better datasets for the Indian context should be stimulated, and sector-specific datasets should be identified to enable the creation and evaluation of fair models. This will allow users to assess the fairness and bias of their models across standard datasets.

2. **External expertise**: The committee should have a mix of both official and non-official members, including experts from industry and academia, who can bring in valuable insights on operationalizing responsible AI principles in practice. Non-official members could represent the interests of AI developers, deployers, data providers, data principals, and end-users.

3. **Regular meetings**: The committee/group should meet at a regular basis to suggest measures that would enable appropriate measurement of fairness, accountability, and transparency in the Indian context. This includes coordinated steps to respond to identified gaps with the benefit of multiregulatory support.

4. **Policy environment**: The committee should work towards creating a policy environment which enables the use of AI for beneficial use-cases while promoting the development and deployment of responsible AI applications in their respective domains/sectors.

5. **Legal clarity and guidance**: The committee should provide legal clarity and certainty around development and use of AI by issuing joint guidance, harmonizing existing efforts and initiatives around common terminologies and risk inventories. This will help operationalize the responsible AI principles.

6. **Self-regulation**: The committee should encourage demonstrable self-regulation to operationalise the responsible AI principles within their domains/sectors.

In summary, the committee proposes to ensure fairness, accountability, and transparency in AI systems within the Indian context by creating better datasets, involving external expertise, regular meetings, policy environment improvements, providing legal guidance, promoting self-regulation, and harmonizing existing efforts around common terminologies and risk inventories.

**Human Answer:** Ensuring Fairness, Accountability, and Transparency: The committee proposes to ensure fairness, accountability, and transparency in AI systems by: ◦ Promoting the creation of better datasets relevant to the Indian context, which allows users to assess the fairness and bias of their models across standard datasets. ◦ Identifying sector-specific datasets to enable the creation and evaluation of fair models.

**Scores:** Similarity: 0.89, ROUGE-L: 0.222, BLEU: 0.123, METEOR: 0.449, BERT Precision: 0.827, BERT Recall: 0.916, BERT F1: 0.869

**Question:** Who are the proposed members of the AI Governance Committee/Group, and why is it important to include both official and non-official members?

**LLM Answer:**  The proposed members of the AI Governance Committee/Group include both official and non-official members. Official members could be representatives deputed from MeitY, the NITI Aayog, the Telecommunication Engineering Centre, Bureau of Indian Standards, other departments of the Central Government, as well as sectoral regulators (e.g., RBI, Indian Council of Medical Research, SEBI, IRDAI, Telecom Regulatory Authority of India, etc.). Non-official members could include persons capable of representing the interests of AI developers, AI deployers, data providers, data principals, and end-users – so that the perspectives of the overall ecosystem can be considered.

It is important to include both official and non-official members because such a forum focused on coordinating AI governance must also bring in external expertise from industry and academia. These stakeholders play a central role in operationalizing responsible AI principles in practice, ensuring that the committee's decisions and recommendations are well-informed and practical. Additionally, by considering the perspectives of all ecosystem participants (developers, deployers, data providers, data principals, and end-users), the Committee/Group can make more comprehensive decisions, addressing potential issues and gaps in a holistic manner.

**Human Answer:** Proposed Members: The AI Governance Committee/Group should have a mix of both official and non-official members. ◦Official members could include representatives from MeitY, NITI Aayog, the Telecommunication Engineering Centre, Bureau of Indian Standards, other central government departments, and sectoral regulators such as RBI, Indian Council of Medical Research, SEBI, IRDAI, and Telecom Regulatory Authority of India. ◦Non-official members could include individuals representing the interests of AI developers, AI deployers, data providers, data principals, and end-users. ◦It is important to include both official and non-official members to bring in external expertise from industry and academia, given their central role in operationalizing responsible AI principles in practice, and to consider the perspectives of the overall ecosystem1. The Committee/ Group should invite external experts for discussions to understand and take on board diverse perspectives.

**Scores:** Similarity: 0.932, ROUGE-L: 0.644, BLEU: 0.4, METEOR: 0.641, BERT Precision: 0.926, BERT Recall: 0.945, BERT F1: 0.935


--------------------------------------------------------------------------------

Context: The need to enable effective compliance and enforcement of existing laws. 1. Deepfakes/ fakes/ malicious content There are existing legal safeguards/instruments to protect against misuse of foundation models for creating malicious synthetic media (i.e., malicious ‘deepfakes’). In this case, depending upon the context and negative effect of the malicious synthetic media in question, multiple laws can apply. For example: See: Rahul Matthan, The Zone of Mischief (2024) 8 Page 7 of 22 ● Information Technology Act, 2000 (IT Act): Section 66D of the IT Act criminalises the use of computer resources for cheating by personation. Section 66E prescribes the punishment for capturing and publishing or transmitting the image of a private area of any person without his or her consent. Publishing or transmitting obscene material for instance, which could be generated by using deepfake technology is a punishable offence under Section 67A and 67B of the IT Act. ● Indian Penal Code (IPC): In addition to the IT Act, certain harms/cybercrimes perpetuated by AI could also fall under the IPC. For instance, identity theft and cheating by personation are offences under Section 419 (cheating by personation), section 463 and 465 (forgery for the purpose of cheating), section 292 and 294 (selling/circulating/distributing obscene objects), and section 499 (causing harm to reputation/defamation). It is to be noted that the IPC has been recently replaced by the Bharatiya Nyaya (Second) Sanhita (BNS2), and the BNS2 retains these offences. ● Other laws: In addition to the IT Act and IPC / BNS2, there could be more laws depending on the nature of crime or cause of actions involved, like Prevention of Children from Sexual Offences Act, 2012 (section 12) in the event of sexual harassment of children, Juvenile Justice (Care and Protection of Children) Act, 2015 (section 75) for causing harm to the children, the Copyrights Act (section 51), if synthetic content infringes copyrighted work. It is worth noting that existing laws can also require specific measures from platforms and online service providers to detect and remove malicious synthetic media. Under the Information Technology (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021: ● Rule 3(1)(b) requires intermediaries to inform its rules and regulations, privacy policy and user agreement to a user and to make reasonable efforts to prevent the dissemination of specific types of content, that may cause harm to its users – including information that may violate bodily privacy, cause harm to a child, is deceptive, among other things. ● Rule 3(1)(c) requires intermediaries to periodically inform their users about the effects of non-compliance with the rules and regulations, privacy policy, or user agreement of such intermediary. ● Rule 3(2)(b) requires the intermediary to, within 24 hours of receiving user complaint of content which is ‘impersonation in electronic form, including artificially morphed images of such individual’ remove or disable its access. Further, the grievance officer appointed by the intermediary should acknowledge user complaints within 24 hours. The above shows that the legal framework may be adequate for the purposes of detecting, preventing, removing, and prosecuting the creation and distribution of malicious synthetic media. However, for this legal framework to be effective, it needs to be backed by requisite capabilities to enable stakeholders to effectively comply and for the authorities to enforce the legal framework. Page 8 of 22 This area points to possible gaps and opportunities for using technological measures for enabling effective compliance, so that malicious deepfakes are detected in time and/ or are removed before they cause serious harm. For example, as suggested above, traceability may be established by assigning unique and immutable identities to different participants, such as content creators, publishers, social media platforms, etc. These may then be used to watermark inputs to, and outputs from, generative AI tools. These may be used to track and analyse the lifecycle, from creation to use, of a deepfake – and to determine when they have been cre

**Question:** How do the Information Technology Act, 2000, and the Indian Penal Code (or its replacement, BNS2) address cybercrimes related to deepfakes?

**LLM Answer:**  The Information Technology Act, 2000 (IT Act) and the Indian Penal Code (IPC) address cybercrimes related to deepfakes by imposing penalties for various offenses that could be perpetuated by AI. Here are some examples:

1. Section 66D of the IT Act criminalizes the use of computer resources for cheating by personation. This can potentially apply to the creation and distribution of deepfakes.
2. Section 66E prescribes punishment for capturing and publishing or transmitting the image of a private area of any person without his or her consent, which could include deepfake images or videos.
3. Publishing or transmitting obscene material, including content generated by using deepfake technology, is a punishable offence under Section 67A and 67B of the IT Act.
4. In addition to the IT Act, certain harms/cybercrimes perpetuated by AI could also fall under the IPC. For instance, identity theft and cheating by personation are offenses under Section 419 (cheating by personation), section 463 and 465 (forgery for the purpose of cheating), section 292 and 294 (selling/circulating/distributing obscene objects), and section 499 (causing harm to reputation/defamation). The Bharatiya Nyaya (Second) Sanhita (BNS2), which has replaced the IPC, retains these offences.

It is worth noting that the IT Act and IPC can potentially apply to deepfakes depending on their content and context. However, as the passage suggests, there could be more laws depending on the nature of crime or cause of actions involved, like the Prevention of Children from Sexual Offences Act, 2012 (section 12) in the event of sexual harassment of children, the Juvenile Justice (Care and Protection of Children) Act, 2015 (section 75) for causing harm to children, and the Copyrights Act (section 51), if synthetic content infringes copyrighted work.

**Human Answer:** The Information Technology Act, 2000 (IT Act), addresses cybercrimes related to deepfakes through several sections:• Section 66D criminalizes the use of computer resources for cheating by personation. This could apply if a deepfake is used to impersonate someone for fraudulent purposes.• Section 66E prescribes punishment for capturing and publishing or transmitting the image of a private area of any person without their consent. Deepfakes creating such images could fall under this section.• Sections 67A and 67B make publishing or transmitting obscene material punishable offences1. This could include obscene content generated using deepfake technology. The Indian Penal Code (IPC) and its recent replacement, the Bharatiya Nyaya (Second) Sanhita (BNS2), also address harms and cybercrimes that could be perpetrated by AI-generated malicious synthetic media: • Section 419 (cheating by personation) of the IPC criminalizes identity theft and cheating by personation1. This offense is retained in the BNS2. •Sections 463 and 465 (forgery for the purpose of cheating) of the IPC address forgery with the intent to deceive1. These offences are also retained in the BNS2. • Sections 292 and 294 (selling/circulating/distributing obscene objects) of the IPC criminalize the dissemination of obscene material. These offences are retained in the BNS2. • Section 499 (causing harm to reputation/defamation) of the IPC addresses defamation1. This offence is also retained in the BNS2. •In addition to these, other laws like the Prevention of Children from Sexual Offences Act, 2012 (section 12), can apply in cases of sexual harassment of children, and the Juvenile Justice (Care and Protection of Children) Act, 2015 (section 75), for causing harm to children1. The Copyrights Act (section 51) may also be relevant if synthetic content infringes copyrighted work

**Scores:** Similarity: 0.927, ROUGE-L: 0.542, BLEU: 0.468, METEOR: 0.619, BERT Precision: 0.922, BERT Recall: 0.92, BERT F1: 0.921

**Question:** What obligations do online intermediaries have under the IT (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021, in detecting and removing malicious synthetic media?

**LLM Answer:**  The Intermediary Guidelines and Digital Media Ethics Code Rules, 2021 outline several obligations for online intermediaries regarding the detection and removal of malicious synthetic media.

  1. **Rule 3(1)(b)**: Online intermediaries are required to inform users about their rules, regulations, privacy policy, and user agreement, and make reasonable efforts to prevent the dissemination of specific types of content that may harm their users, such as information that violates bodily privacy, causes harm to a child, is deceptive, among other things.

  2. **Rule 3(1)(c)**: Online intermediaries must periodically inform their users about the consequences of non-compliance with their rules, regulations, privacy policy, or user agreement.

  3. **Rule 3(2)(b)**: In response to a complaint from a user regarding content that is "impersonation in electronic form, including artificially morphed images" (such as malicious deepfakes), the intermediary must remove or disable the content's access within 24 hours. The grievance officer appointed by the intermediary should acknowledge the complaint within this same timeframe.

**Human Answer:** Under the Information Technology (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021, online intermediaries have specific obligations in detecting and removing malicious synthetic media: •Rule 3(1)(b) requires intermediaries to inform users of their rules, regulations, privacy policy, and user agreement and to make reasonable efforts to prevent the dissemination of specific types of harmful content. This includes information that may violate bodily privacy, cause harm to a child, or is deceptive. •Rule 3(1)(c) mandates that intermediaries periodically inform their users about the consequences of non-compliance with their rules, regulations, privacy policy, or user agreement. •Rule 3(2)(b) requires intermediaries to remove or disable access to content within 24 hours of receiving a user complaint regarding 'impersonation in electronic form, including artificially morphed images of such individual'. Furthermore, the grievance officer appointed by the intermediary must acknowledge user complaints within 24 hour.

**Scores:** Similarity: 0.968, ROUGE-L: 0.703, BLEU: 0.429, METEOR: 0.687, BERT Precision: 0.931, BERT Recall: 0.949, BERT F1: 0.94

**Question:** Why is technological intervention necessary for the effective enforcement of laws against malicious deepfakes?

**LLM Answer:**  The text provides several reasons why technological intervention is necessary for the effective enforcement of laws against malicious deepfakes. Here are some key points:

1. Establishing traceability: Unique and immutable identities could be assigned to different participants, such as content creators, publishers, social media platforms, etc. These identities can be used to watermark inputs and outputs from generative AI tools, enabling the lifecycle of a deepfake to be tracked and analyzed from creation to use.

2. Preventing dissemination: Intermediaries are required to make reasonable efforts to prevent the dissemination of content that may cause harm, including information that violates bodily privacy, causes harm to children, or is deceptive (among other things). This requires technological capabilities to detect and remove malicious deepfakes before they cause serious harm.

3. Enforcing removal: The text mentions a rule that requires intermediaries to remove or disable access to content involving impersonation in electronic form within 24 hours of receiving a complaint. To effectively enforce this rule, technological tools are necessary to detect and remove such content swiftly.

4. Compliance with legal framework: The legal framework already exists for the detection, prevention, removal, and prosecution of malicious deepfakes. However, for this framework to be effective, it needs to be backed by requisite capabilities that enable stakeholders to effectively comply and for authorities to enforce the laws.

In summary, technological intervention is necessary for the effective enforcement of laws against malicious deepfakes due to the need for traceability, preventing dissemination, enforcing removal, and ensuring compliance with existing legal frameworks.

**Human Answer:** While the existing legal framework may be adequate for detecting, preventing, removing, and prosecuting the creation and distribution of malicious synthetic media, technological intervention is necessary for its effective enforcement. This is because technological measures can enable stakeholders to effectively comply with the law, and authorities to enforce it by detecting malicious deepfakes in time and/or removing them before they cause serious harm.

**Scores:** Similarity: 0.778, ROUGE-L: 0.158, BLEU: 0.066, METEOR: 0.476, BERT Precision: 0.844, BERT Recall: 0.893, BERT F1: 0.868

**Question:** How can traceability and watermarking help in tracking and preventing the spread of harmful AI-generated content?

**LLM Answer:**  Traceability and watermarking can help in tracking and preventing the spread of harmful AI-generated content by establishing a unique and immutable identity for different participants involved, such as content creators, publishers, and social media platforms. These identities can be used to watermark inputs and outputs from generative AI tools.

   The watermarks serve as digital fingerprints that help track the lifecycle of an AI-generated deepfake, from its creation to its use. This information can be utilized to detect when deepfakes have been created maliciously and are being spread with harmful intentions.

   By having this traceability mechanism in place, authorities can more effectively enforce existing laws by identifying the parties responsible for creating and distributing malicious synthetic media. It also aids online service providers in making reasonable efforts to prevent the dissemination of such content, as required under guidelines like the Information Technology (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021.

   Overall, traceability and watermarking are crucial technological measures for enabling effective compliance with legal frameworks aimed at detecting, preventing, removing, and prosecuting the creation and distribution of malicious synthetic media.

**Human Answer:** Traceability and watermarking are suggested as technological measures that can help in tracking and preventing the spread of harmful AI-generated content1. Establishing unique and immutable identities for participants such as content creators, publishers, and social media platforms, and using these to watermark inputs and outputs of generative AI tools, could allow for tracking and analyzing the lifecycle of a deepfake from creation to use. This can help in determining when harmful content has been created and disseminated.

**Scores:** Similarity: 0.852, ROUGE-L: 0.431, BLEU: 0.177, METEOR: 0.503, BERT Precision: 0.88, BERT Recall: 0.94, BERT F1: 0.909


--------------------------------------------------------------------------------
