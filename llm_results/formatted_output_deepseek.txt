# Context: 

The Digital Personal Data Protection Bill, 2023 was introduced in Lok Sabha on August 3, 2023. The Bill seeks to provide for the protection of personal data and the privacy of individuals. Applicability: The Bill applies to the processing of digital personal data within India where such data is: (i) collected online, or (ii) collected offline and is digitised. It will also apply to the processing of personal data outside India if it is for offering goods or services in India. Personal data is defined as any data about an individual who is identifiable by or in relation to such data. Processing has been defined as wholly or partially automated operation or set of operations performed on digital personal data. It includes collection, storage, use, and sharing. Consent: Personal data may be processed only for a lawful purpose after obtaining the consent of the individual. A notice must be given before seeking consent. The notice should contain details about the personal data to be collected and the purpose of processing. Consent may be withdrawn at any point in time. Consent will not be required for 'legitimate uses' including: (i) specified purpose for which data has been provided by an individual voluntarily, (ii) provision of benefit or service by the government, (iii) medical emergency, and (iv) employment. For individuals below 18 years of age, consent will be provided by the parent or the legal guardian. Rights and duties of data principal: An individual, whose data is being processed (data principal), will have the right to: (i) obtain information about processing, (ii) seek correction and erasure of personal data, (iii) nominate another person to exercise rights in the event of death or incapacity, and (iv) grievance redressal. Data principals will have certain duties. They must not: (i) register a false or frivolous complaint, and (ii) furnish any false particulars or impersonate another person in specified cases. Violation of duties will be punishable with a penalty of up to Rs 10,000. Obligations of data fiduciaries: The entity, determining the purpose and means of processing, (data fiduciary), must: (i) make reasonable efforts to ensure the accuracy and completeness of data, (ii) build reasonable security safeguards to prevent a data breach, (iii) inform the Data Protection Board of India and affected persons in the event of a breach, and (iv) erase personal data as soon as the purpose has been met and retention is not necessary for legal purposes (storage limitation). In case of government entities, storage limitation and the right of the data principal to erasure will not apply. Significant data fiduciaries: Certain data fiduciaries may be designated as significant data fiduciaries. Certain factors must be taken into regard such as: (i) volume and sensitivity of personal data processed, (ii) risks to the rights of data principals, (iii) security of the state, and (iv) public order. These entities will have certain additional obligations including: (i) appointing a data protection officer, and (ii) undertaking impact assessment and compliance audit. Exemptions: Rights of the data principal and obligations of data fiduciaries (except data security) will not apply in specified cases. These include: (i) prevention and investigation of offences, and (ii) enforcement of legal rights or claims. The central government may, by notification, exempt certain activities from the application of the Bill. These include: (i) processing by government entities in the interest of the security of the state and public order, and (ii) research, archiving, or statistical purposes. Processing of personal data of children: While processing the personal data of a child, the data fiduciary must not undertake: (i) processing that is likely to cause any detrimental effect on the well-being of the child, and (ii) tracking, behavioural monitoring, or targeted advertising. Cross-border transfer: The Bill allows the transfer of personal data outside India, except to countries restricted by the government through notification. Data Protection Board of India: The central government will establish the Data Protection Board of India. Key functions of the Board include: (i) monitoring compliance and imposing penalties, (ii) directing data fiduciaries to take necessary measures in the event of a data breach, and (iii) hearing grievances made by affected persons. Board members will be appointed for two years and will be eligible for re-appointment. Penalties: The schedule to the Bill specifies penalties for various offences such as up to: (i) Rs 200 crore for non-fulfilment of obligations for children, and (ii) Rs 250 crore for failure to take security measures to prevent data breaches.

**Question:** What are the key applicability criteria for the Digital Personal Data Protection Bill, 2023?

**LLM Answer:** The key applicability criteria for the Digital Personal Data Protection Bill, 2023 are as follows:

1. **Scope of Application**: 
   - Applies to digital personal data collected online or offline (after digitization).
   - Extends to processing outside India when providing goods/services in India.

2. **Personal Data Definition**:
   - Consists of any identifiable information about an individual.

3. **Processing Activities**:
   - Includes collection, storage, use, and sharing, both entirely and partially automated.

4. **Consent Requirements**:
   - Personal data processing is permissible only with lawful consent.
   - A mandatory notice must be provided detailing the purpose, data collected, and usage methods.
   - Consent not required for legitimate purposes such as government services, medical emergencies, or employment.

5. **Rights of Data Principal**:
   - Individuals have rights to information, correction, erasure, proxy nomination, and grievance redressal.
   - Obligations include preventing false claims, providing accurate details, and compliance with specific prohibitions regarding children's data processing.

6. **Duties of Data Fiduciaries**:
   - Ensure data accuracy, implement security measures against breaches, inform affected parties on breaches, and delete data once its purpose is met (except for legal requirements).
   - Exemptions apply in cases like government entities' activities or research for public interest.

7. **Significant Data Fiduciaries**:
   - Designated by factors such as data sensitivity, impact risk, state security, etc.
   - Additional responsibilities include appointing a data protection officer and conducting audits.

8. **Exemptions and Prohibitions**:
   - Exemptions exist for certain rights in specific scenarios (e.g., preventing crimes).
   - Prohibitions on processing personal data related to children's welfare or activities affecting public order.

9. **Cross-Border Processing**:
   - Allowed except in countries notified as restricted by government.

10. **Supervisory Measures**:
    - The Data Protection Board is established with functions to monitor compliance, impose penalties, and address grievances.
    - Appointed members can be re-elected for two years each.

This structured approach ensures clarity on the bill's applicability across various data handling scenarios while outlining accountability measures through designated fiduciaries and supervisory bodies.

**Human Answer:** Applicability Criteria: The Bill applies to the processing of digital personal data within India if the data is (i) collected online, or (ii) collected offline and then digitized. It also applies to processing personal data outside India if it involves offering goods or services within India.

**Scores:** Similarity: 0.794, ROUGE-L: 0.118, BLEU: 0.007, METEOR: 0.275, BERT Precision: 0.773, BERT Recall: 0.885, BERT F1: 0.825

**Question:** Under what conditions can personal data be processed without the consent of the individual?

**LLM Answer:** Under the Digital Personal Data Protection Bill of 2023, personal data may be processed without consent under specific legitimate conditions:

1. **Legitimate Purposes**: Processing is allowed when conducted for purposes such as providing goods or services by the government, medical emergencies, employment benefits, or other specified reasons.

2. **Government Functions**: Activities involving the collection of data to prevent or investigate crimes, enforce legal rights, or serve public interest or security do not require consent.

3. **Children's Consent**: Parents or guardians can consent on behalf of minors (under 18 years) for any purpose, and this consent remains valid if continued after data collection.

4. **Exempted Activities**: The Bill exempts certain activities from its application, including research, archiving, statistical purposes, and others outlined in the text.

These exceptions are designed to balance privacy rights with necessary data processing, allowing information to be shared when justified or required by law.

**Human Answer:** Conditions for Processing Data Without Consent: Personal data can be processed without individual consent for 'legitimate uses,' including when data is voluntarily provided for a specified purpose, for the provision of government benefits or services, during medical emergencies, and for employment purposes.

**Scores:** Similarity: 0.788, ROUGE-L: 0.179, BLEU: 0.015, METEOR: 0.353, BERT Precision: 0.838, BERT Recall: 0.906, BERT F1: 0.871

**Question:** What rights are granted to data principals under the Bill?

**LLM Answer:** Under the Digital Personal Data Protection Bill, 2023, individuals whose personal data is being processed (referred to as data principals) are granted specific rights:

1. **Right to Information**: The data principal has the right to obtain information about how their data is being processed.

2. **Right to Correction and Deletion**: They can request corrections or erasure of inaccurate personal data at any time.

3. **Right of Nominating Proxy**: In cases where they are incapacitated or have died, another person may be nominated to exercise the aforementioned rights.

4. **Grievance Redressal**: Data principals have a grievance redressal mechanism available to them for addressing issues arising from data processing activities.

These rights ensure that individuals can access, review, and manage their personal data within the legal framework provided by the Bill.

**Human Answer:** Rights of Data Principals: Data principals, or individuals whose data is being processed, have the right to (i) obtain information about the processing of their data, (ii) seek correction and erasure of their personal data, (iii) nominate someone to exercise their rights if they die or become incapacitated, and (iv) grievance redressal.

**Scores:** Similarity: 0.847, ROUGE-L: 0.298, BLEU: 0.053, METEOR: 0.492, BERT Precision: 0.854, BERT Recall: 0.883, BERT F1: 0.868

**Question:** What obligations do data fiduciaries have in ensuring data protection and security?

**LLM Answer:** The obligations of data fiduciaries under the Digital Personal Data Protection Bill, 2023, are comprehensive and designed to ensure robust protection and security of personal data. Here's a structured summary of these obligations:

1. **Duties of Data Fiduciaries:**
   - **Accuracy and Completeness:** Ensure all data is accurate and complete.
   - **Security Measures:** Implement safeguards against breaches.
   - **Breach Reporting:** Inform the Data Protection Board (DPRB) and affected individuals upon a breach.
   - **Data Erasure:** Erase personal data once its purpose is fulfilled unless required by law.

2. **Significant Data Fiduciaries:**
   - These entities, characterized by processing large sensitive data or posing high risks to data rights, must designate a Data Protection Officer (DPO).
   - They are required to conduct impact assessments and compliance audits as part of their obligations.

3. **Exemptions and Penalties:**
   - Exemptions apply in cases such as preventing crimes or enforcing legal rights.
   - Penalties for non-compliance include fines up to Rs 200 crore for children's data issues and Rs 250 crore for security breaches, imposed by the DPRB.

This framework ensures that data fiduciaries are held accountable for protecting personal data while providing a clear pathway for exemptions where applicable.

**Human Answer:** Obligations of Data Fiduciaries: Data fiduciaries, entities determining the purpose and means of data processing, must (i) make reasonable efforts to ensure data accuracy and completeness, (ii) build reasonable security safeguards to prevent data breaches, (iii) inform the Data Protection Board of India and affected individuals of data breaches, and (iv) erase personal data when its purpose has been met and retention is no longer legally necessary. Note that storage limitation and the right of the data principal to erasure do not apply to government entities.

**Scores:** Similarity: 0.824, ROUGE-L: 0.255, BLEU: 0.058, METEOR: 0.375, BERT Precision: 0.821, BERT Recall: 0.869, BERT F1: 0.844

**Question:** What are the penalties specified in the Bill for non-compliance with data protection obligations?

**LLM Answer:** The Digital Personal Data Protection Bill, 2023 specifies the following penalties for non-compliance with data protection obligations:

1. **Rs 200 Crore Penalty**: For non-fulfilment of obligations specifically regarding children's personal data.

2. **Rs 250 Crore Penalty**: For failure to implement adequate security measures resulting in a data breach.

These penalties are outlined under the "Penalties" section of the Bill, focusing on specific offenses related to data handling and security breaches.

**Human Answer:** Penalties for Non-Compliance: The Bill specifies penalties for various offenses, including (i) up to Rs 200 crore for not fulfilling obligations related to children's data, and (ii) up to Rs 250 crore for failing to implement security measures to prevent data breaches

**Scores:** Similarity: 0.891, ROUGE-L: 0.427, BLEU: 0.036, METEOR: 0.471, BERT Precision: 0.884, BERT Recall: 0.915, BERT F1: 0.899


--------------------------------------------------------------------------------

# Context: 

The committee recommends the following: 1. To implement a whole-of-government approach to AI Governance, MeitY and the Principal Scientific Adviser should establish an empowered mechanism to coordinate AI Governance. The empowered mechanism should be in the form of an Inter-Ministerial AI Coordination Committee or Governance Group (Committee/ Group). It should bring together the various authorities and institutions that deal with AI Governance at the national level. The Committee/ Group should have an ongoing status and should not be a limited duration mechanism. The overall purpose of this Committee/ Group should be to bring the key institutions around a common roadmap and to coordinate their efforts to implement a whole-of government approach. A collaborative and co-ordinated approach by various regulators can enable them to be more efficient and effective, given the complexity likely to be involved in dealing with AI systems at scale, especially when we take a long-term view of the diffusion stage of their lifecycle. This can be especially necessary in domains and areas where multiple authorities may be concerned (e.g., consumer protection, food, transportation, agriculture, health care, etc.). The Committee/ Group should enable a whole of government approach to the AI ecosystem. Currently, regulators and government departments may have some visibility on the AI systems developed or deployed by entities who are under sectoral regulation (e.g., finance or health) or where the market is concentrated (e.g., ecommerce, social media, aggregators). However, the level of visibility would need to be adequate to assess potential risks associated with such entities in the context of AI. Further, there are likely to be AI systems developed or being developed and/ or deployed by entities who may not have an interface with the government/ regulators from a perspective of affording suitable visibility to enable a risk assessment in relation to AI. A pre-requisite of governance would be for the government and the regulators to have a credible understanding the AI ecosystem in the country so that governance measures are rooted to the realities of existing and likely risks. The Committee/ Group should facilitate this task. This would require a conversation-led approach with a view to develop an understanding of the ecosystem which can both serve as feedback for strengthening governance and enable understanding of possible challenges and gaps in complying Page 13 of 22 and enforcing existing laws. It is important to emphasise that such a mapping exercise should not result in regulatory overreach through at scale registration and reporting requirements. With the above context, the Committee/ Group should meet at a regular basis to suggest measures to catalyse collaboration between departments and regulators, so that they can: • apply and strengthen existing laws to minimise risk of harm due to use of AI; • provide legal clarity and certainty around development and use of AI by issuing joint guidance; • harmonise existing efforts and initiatives around common terminologies and risk inventories; • enable demonstrable self-regulation to operationalise the responsible AI principles; • take coordinated steps to respond to identified gaps with the benefit of multiregulatory support; • create a policy environment which enables the use of AI for beneficial use-cases; and • promote the development and deployment of responsible AI applications in their domains/sectors. In order to enable appropriate measurement of fairness, accountability and transparency in the Indian context, it is an essential prerequisite to have access to the right datasets, relevant to the Indian context, which allows users to assess the fairness and bias of their models across standard datasets. The creation of better datasets for the Indian context should be stimulated, and sector-specific datasets should be identified to enable creation and evaluation of fair models. These initiatives may be encouraged by the Committee/ Group. The Committee/ Group should have a mix of both official and non-official members, because such a forum focused on coordinating AI governance must also bring in external expertise from industry and academia, given their central role in operationalising responsible AI principles in practice. It may be headed by the Principal Scientific Adviser. Official members could include representatives deputed from MeitY, the NITI Aayog, the Telecommunication Engineering Centre, Bureau of Indian Standards, other departments of the Central Government, as well as sectoral regulators (e.g., RBI, Indian Council of Medical Research, SEBI, IRDAI, Telecom Regulatory Authority of India, etc.). Non-official members could include persons capable of representing the interests of AI developers, AI deployers, data providers, data principals, and end-users – so that the perspectives of the overall ecosystem can be considered. The Committee/ Group should invite external experts for discussions to understand and take on board diverse perspectives.

**Question:** What is the primary objective of the proposed Inter-Ministerial AI Coordination Committee or Governance Group?

**LLM Answer:** The primary objective of the Inter-Ministerial AI Coordination Committee or Governance Group is to implement a whole-of-government approach to AI governance by coordinating efforts among various authorities. This coordination aims to streamline operations, enhance efficiency, and ensure compliance with existing laws while addressing challenges across different sectors such as finance, healthcare, transportation, and agriculture. The committee seeks to foster collaboration between departments and regulators to improve risk assessment, visibility into AI systems, and the creation of a supportive policy environment for responsible AI use.

**Answer:**  
The Inter-Ministerial AI Coordination Committee or Governance Group's primary objective is to establish a comprehensive mechanism that brings together various government institutions to effectively manage AI systems across different sectors through coordinated efforts. This coordination aims to enhance efficiency, ensure compliance with laws, address sectoral challenges, and promote responsible AI deployment by fostering collaboration and shared understanding among regulators and departments.

**Human Answer:** Primary Objective: The primary objective of the proposed Inter-Ministerial AI Coordination Committee or Governance Group is to bring key institutions together around a common roadmap and to coordinate their efforts to implement a whole-of-government approach to AI governance1. This includes enabling a whole-of-government approach to the AI ecosystem.

**Scores:** Similarity: 0.876, ROUGE-L: 0.275, BLEU: 0.115, METEOR: 0.467, BERT Precision: 0.867, BERT Recall: 0.92, BERT F1: 0.893

**Question:** Why is a whole-of-government approach considered essential for AI governance in India?

**LLM Answer:** A whole-of-government approach is essential for AI governance in India because it fosters coordinated efforts among various agencies to manage AI systems effectively. This centralized coordination helps address potential risks by ensuring oversight across multiple domains such as finance, healthcare, and transportation. The Inter-Ministerial AI Coordination Committee or Governance Group will facilitate understanding the entire AI ecosystem through regular meetings, enabling better risk management and compliance with laws. External expertise from different sectors ensures a comprehensive approach, considering diverse perspectives to promote responsible AI use. This collaborative framework helps avoid regulatory overreach while ensuring legal clarity and accountability, promoting innovation and ethical AI deployment in India's unique context.

**Human Answer:** Importance of a Whole-of-Government Approach: A whole-of-government approach is considered essential because a collaborative and coordinated approach by various regulators can enable them to be more efficient and effective, given the complexity likely to be involved in dealing with AI systems at scale, especially in the long term1. This is particularly necessary in areas where multiple authorities may be concerned, such as consumer protection, food, transportation, agriculture, and health care.

**Scores:** Similarity: 0.628, ROUGE-L: 0.204, BLEU: 0.04, METEOR: 0.309, BERT Precision: 0.867, BERT Recall: 0.886, BERT F1: 0.876

**Question:** What are the key functions and responsibilities of the AI Governance Committee/Group?

**LLM Answer:** The AI Governance Committee/Group is tasked with several critical functions to ensure effective AI governance within India:

1. **Coordination and Collaboration**: The committee coordinates AI governance across various government departments and institutions, aiming for a whole-of-government approach to address challenges in the AI ecosystem.

2. **Legal Application and Compliance**: It ensures that existing laws are applied effectively to minimize risks associated with AI use, providing legal clarity through joint guidance and harmonizing efforts across initiatives.

3. **Policy Environment Creation**: The committee creates a policy environment that encourages responsible AI use, promotes transparency, fairness, accountability, and self-regulation among stakeholders.

4. **Data Accessibility and Quality**: It focuses on improving datasets relevant to India's context to assess AI model fairness and bias, ensuring better data availability for AI development.

5. **Regular Meetings and Reporting**: The committee meets regularly to suggest measures for collaboration between departments and regulators, applying these measures as needed.

6. **External Expert Contributions**: To gather diverse perspectives, the committee invites external experts, representing various stakeholders such as AI developers and regulators.

In essence, the AI Governance Committee/Group is pivotal in fostering a comprehensive, compliant, and responsible AI ecosystem through coordinated efforts, legal adherence, policy promotion, and stakeholder collaboration.

**Human Answer:** Key Functions and Responsibilities: The AI Governance Committee/Group is responsible for: 1) Suggesting measures to catalyze collaboration between departments and regulators. 2) Applying and strengthening existing laws to minimize the risk of harm due to the use of AI. 3) Providing legal clarity and certainty around the development and use of AI by issuing joint guidance.4) Harmonizing existing efforts and initiatives around common terminologies and risk inventories. 5) Enabling demonstrable self-regulation to operationalize responsible AI principles. 6) Taking coordinated steps to respond to identified gaps with multi-regulatory support. 7) Creating a policy environment that enables the use of AI for beneficial use-cases. 8) Promoting the development and deployment of responsible AI applications in various domains and sectors.

**Scores:** Similarity: 0.826, ROUGE-L: 0.245, BLEU: 0.068, METEOR: 0.356, BERT Precision: 0.853, BERT Recall: 0.881, BERT F1: 0.867

**Question:** How does the committee propose to ensure fairness, accountability, and transparency in AI systems within the Indian context?

**LLM Answer:** The committee's proposed framework for ensuring fairness, accountability, and transparency in AI systems within India is structured around several key components:

1. **Whole-of-Government Approach**: An Inter-Ministerial AI Coordination Committee or Governance Group will be established to coordinate AI governance across various agencies. This ongoing committee aims to align institutions around a common roadmap, enhance efficiency by addressing complex issues collectively, and ensure alignment with sectors like finance, healthcare, etc.

2. ** datasets for Fairness and Transparency**: Quality datasets specific to the Indian context will be created and promoted. These datasets will help assess AI models' fairness and bias across standard datasets, ensuring accurate evaluation in various fields such as finance, healthcare, and more.

3. **External Expert Involvement**: The governance group will include both official and external experts from academia and industry. This diversity of perspectives is crucial for comprehensive policy-making and effective AI deployment.

4. **Regular Meetings and Collaborative Efforts**: The committee will meet regularly to suggest measures that foster collaboration between departments and regulators. This includes applying existing laws, issuing joint guidance, harmonizing efforts, enabling self-regulation, responding to gaps, creating a beneficial policy environment, and promoting responsible AI use.

In summary, the framework emphasizes a coordinated governance model with external expertise and regular meetings to ensure fairness, accountability, and transparency in AI systems within India.

**Human Answer:** Ensuring Fairness, Accountability, and Transparency: The committee proposes to ensure fairness, accountability, and transparency in AI systems by: ◦ Promoting the creation of better datasets relevant to the Indian context, which allows users to assess the fairness and bias of their models across standard datasets. ◦ Identifying sector-specific datasets to enable the creation and evaluation of fair models.

**Scores:** Similarity: 0.846, ROUGE-L: 0.209, BLEU: 0.088, METEOR: 0.382, BERT Precision: 0.834, BERT Recall: 0.899, BERT F1: 0.866

**Question:** Who are the proposed members of the AI Governance Committee/Group, and why is it important to include both official and non-official members?

**LLM Answer:** The proposed AI Governance Committee/Group is structured with both official and non-official members to facilitate effective coordination across various stakeholders. The official members include key government institutions such as MeitY, NITI Aayog, Telecommunication Engineering Centre, Bureau of Indian Standards, sectoral regulators (RBI, ICMR, SEBI, IRDAI, TRAI), and other relevant departments. 

Non-official members are comprised of external experts representing AI developers, deployers, data providers, data principals, and end-users. This composition ensures diverse perspectives from the broader AI ecosystem.

In summary, the committee's membership is a strategic blend of internal government representatives and external experts to address various aspects of AI governance comprehensively.

**Human Answer:** Proposed Members: The AI Governance Committee/Group should have a mix of both official and non-official members. ◦Official members could include representatives from MeitY, NITI Aayog, the Telecommunication Engineering Centre, Bureau of Indian Standards, other central government departments, and sectoral regulators such as RBI, Indian Council of Medical Research, SEBI, IRDAI, and Telecom Regulatory Authority of India. ◦Non-official members could include individuals representing the interests of AI developers, AI deployers, data providers, data principals, and end-users. ◦It is important to include both official and non-official members to bring in external expertise from industry and academia, given their central role in operationalizing responsible AI principles in practice, and to consider the perspectives of the overall ecosystem1. The Committee/ Group should invite external experts for discussions to understand and take on board diverse perspectives.

**Scores:** Similarity: 0.917, ROUGE-L: 0.43, BLEU: 0.253, METEOR: 0.435, BERT Precision: 0.923, BERT Recall: 0.914, BERT F1: 0.918


--------------------------------------------------------------------------------

# Context: 

The need to enable effective compliance and enforcement of existing laws. 1. Deepfakes/ fakes/ malicious content There are existing legal safeguards/instruments to protect against misuse of foundation models for creating malicious synthetic media (i.e., malicious ‘deepfakes’). In this case, depending upon the context and negative effect of the malicious synthetic media in question, multiple laws can apply. For example: See: Rahul Matthan, The Zone of Mischief (2024) 8 Page 7 of 22 ● Information Technology Act, 2000 (IT Act): Section 66D of the IT Act criminalises the use of computer resources for cheating by personation. Section 66E prescribes the punishment for capturing and publishing or transmitting the image of a private area of any person without his or her consent. Publishing or transmitting obscene material for instance, which could be generated by using deepfake technology is a punishable offence under Section 67A and 67B of the IT Act. ● Indian Penal Code (IPC): In addition to the IT Act, certain harms/cybercrimes perpetuated by AI could also fall under the IPC. For instance, identity theft and cheating by personation are offences under Section 419 (cheating by personation), section 463 and 465 (forgery for the purpose of cheating), section 292 and 294 (selling/circulating/distributing obscene objects), and section 499 (causing harm to reputation/defamation). It is to be noted that the IPC has been recently replaced by the Bharatiya Nyaya (Second) Sanhita (BNS2), and the BNS2 retains these offences. ● Other laws: In addition to the IT Act and IPC / BNS2, there could be more laws depending on the nature of crime or cause of actions involved, like Prevention of Children from Sexual Offences Act, 2012 (section 12) in the event of sexual harassment of children, Juvenile Justice (Care and Protection of Children) Act, 2015 (section 75) for causing harm to the children, the Copyrights Act (section 51), if synthetic content infringes copyrighted work. It is worth noting that existing laws can also require specific measures from platforms and online service providers to detect and remove malicious synthetic media. Under the Information Technology (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021: ● Rule 3(1)(b) requires intermediaries to inform its rules and regulations, privacy policy and user agreement to a user and to make reasonable efforts to prevent the dissemination of specific types of content, that may cause harm to its users – including information that may violate bodily privacy, cause harm to a child, is deceptive, among other things. ● Rule 3(1)(c) requires intermediaries to periodically inform their users about the effects of non-compliance with the rules and regulations, privacy policy, or user agreement of such intermediary. ● Rule 3(2)(b) requires the intermediary to, within 24 hours of receiving user complaint of content which is ‘impersonation in electronic form, including artificially morphed images of such individual’ remove or disable its access. Further, the grievance officer appointed by the intermediary should acknowledge user complaints within 24 hours. The above shows that the legal framework may be adequate for the purposes of detecting, preventing, removing, and prosecuting the creation and distribution of malicious synthetic media. However, for this legal framework to be effective, it needs to be backed by requisite capabilities to enable stakeholders to effectively comply and for the authorities to enforce the legal framework. Page 8 of 22 This area points to possible gaps and opportunities for using technological measures for enabling effective compliance, so that malicious deepfakes are detected in time and/ or are removed before they cause serious harm. For example, as suggested above, traceability may be established by assigning unique and immutable identities to different participants, such as content creators, publishers, social media platforms, etc. These may then be used to watermark inputs to, and outputs from, generative AI tools. These may be used to track and analyse the lifecycle, from creation to use, of a deepfake – and to determine when they have been cre

**Question:** How do the Information Technology Act, 2000, and the Indian Penal Code (or its replacement, BNS2) address cybercrimes related to deepfakes?

**LLM Answer:** The laws relevant to addressing cybercrimes related to deepfakes include:

1. **Information Technology Act (IT Act) 2000**:
   - Section 66D: Penalizes the use of computer resources for impersonation, which is applicable when deepfakes are used without consent.
   - Section 66E: Targets the publishing or transmission of images or obscene material, relevant if deepfakes spread false or harmful content.
   - Sections 67A and 67B: Address the creation and distribution of false information, fitting scenarios where deepfakes spread misinformation.

2. **Indian Penal Code (IPC) and Bharatiya Nyaya (Second) Sanhita (BNS2)**:
   - Section 419: Covers forgery, which could apply if deepfakes involve impersonation or alteration of facts.
   - Sections 463 and 465: Pertinent for impersonation, relevant to deepfakes that mimic individuals without consent.
   - Sections 292 and 294: Target distributing obscene material, applicable if deepfakes create such content.
   - Section 499: Relates to harm caused by defamation or misinformation from false content.

3. **Additional Considerations**:
   - The IT Act also mentions intermediary guidelines enforcing content moderation, requiring platforms to take action within 24 hours of receiving complaints about deepfakes, suggesting technological safeguards but emphasizing the need for effective enforcement.

In summary, both the IT Act and IPC/BNS2 provide legal frameworks to address deepfake-related crimes by defining prohibited actions and criminalizing them. Intermediary guidelines add layers of protection through content moderation but highlight the importance of robust implementation.

**Human Answer:** The Information Technology Act, 2000 (IT Act), addresses cybercrimes related to deepfakes through several sections:• Section 66D criminalizes the use of computer resources for cheating by personation. This could apply if a deepfake is used to impersonate someone for fraudulent purposes.• Section 66E prescribes punishment for capturing and publishing or transmitting the image of a private area of any person without their consent. Deepfakes creating such images could fall under this section.• Sections 67A and 67B make publishing or transmitting obscene material punishable offences1. This could include obscene content generated using deepfake technology. The Indian Penal Code (IPC) and its recent replacement, the Bharatiya Nyaya (Second) Sanhita (BNS2), also address harms and cybercrimes that could be perpetrated by AI-generated malicious synthetic media: • Section 419 (cheating by personation) of the IPC criminalizes identity theft and cheating by personation1. This offense is retained in the BNS2. •Sections 463 and 465 (forgery for the purpose of cheating) of the IPC address forgery with the intent to deceive1. These offences are also retained in the BNS2. • Sections 292 and 294 (selling/circulating/distributing obscene objects) of the IPC criminalize the dissemination of obscene material. These offences are retained in the BNS2. • Section 499 (causing harm to reputation/defamation) of the IPC addresses defamation1. This offence is also retained in the BNS2. •In addition to these, other laws like the Prevention of Children from Sexual Offences Act, 2012 (section 12), can apply in cases of sexual harassment of children, and the Juvenile Justice (Care and Protection of Children) Act, 2015 (section 75), for causing harm to children1. The Copyrights Act (section 51) may also be relevant if synthetic content infringes copyrighted work

**Scores:** Similarity: 0.877, ROUGE-L: 0.27, BLEU: 0.126, METEOR: 0.285, BERT Precision: 0.859, BERT Recall: 0.861, BERT F1: 0.86

**Question:** What obligations do online intermediaries have under the IT (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021, in detecting and removing malicious synthetic media?

**LLM Answer:** Under the IT (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021, online intermediaries have specific obligations related to detecting and removing malicious synthetic media. These obligations are outlined in **Rule 3(1)(b)**, **Rule 3(1)(c)**, and **Rule 3(2)(b)**:

1. **Rule 3(1)(b):** Online intermediaries must inform users about their rules, regulations, privacy policy, and user agreement. They are required to take reasonable efforts to prevent the dissemination of content that may harm users, including content that violates bodily privacy, causes harm to children, is deceptive, or has other negative effects.

2. **Rule 3(1)(c):** Intermediaries must periodically inform their users about changes in their rules and regulations, privacy policy, or user agreement that could affect the rights of users. This includes transparency about any non-compliance with these guidelines.

3. **Rule 3(2)(b):** If a user complains about content being impersonation in electronic form (including artificially morphed images), the intermediary must remove or disable access to such content within 24 hours. Additionally, the grievance officer appointed by the intermediary should acknowledge the complaint within this timeframe.

These rules ensure that intermediaries have a framework for detecting, preventing, removing, and addressing malicious synthetic media while maintaining accountability and user trust.

**Human Answer:** Under the Information Technology (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021, online intermediaries have specific obligations in detecting and removing malicious synthetic media: •Rule 3(1)(b) requires intermediaries to inform users of their rules, regulations, privacy policy, and user agreement and to make reasonable efforts to prevent the dissemination of specific types of harmful content. This includes information that may violate bodily privacy, cause harm to a child, or is deceptive. •Rule 3(1)(c) mandates that intermediaries periodically inform their users about the consequences of non-compliance with their rules, regulations, privacy policy, or user agreement. •Rule 3(2)(b) requires intermediaries to remove or disable access to content within 24 hours of receiving a user complaint regarding 'impersonation in electronic form, including artificially morphed images of such individual'. Furthermore, the grievance officer appointed by the intermediary must acknowledge user complaints within 24 hour.

**Scores:** Similarity: 0.915, ROUGE-L: 0.558, BLEU: 0.313, METEOR: 0.601, BERT Precision: 0.91, BERT Recall: 0.939, BERT F1: 0.924

**Question:** Why is technological intervention necessary for the effective enforcement of laws against malicious deepfakes?

**LLM Answer:** Technological intervention is essential for enforcing laws against malicious deepfakes due to several key reasons:

1. **Enhanced Detection Mechanisms**: Technology allows for the assignment of unique identifiers to participants involved in creating or distributing content. These identifiers can be used to trace the origin of deepfakes, aiding in their quick identification and removal.

2. **Automation and Efficiency**: AI-powered tools automate detection processes, enabling rapid identification of potential deepfakes among vast amounts of data. This automation minimizes manual oversight, which is often insufficient for detecting closely resemblance deepfakes.

3. **Watermarking and Traceability**: By embedding unique watermarks in AI tools used to create synthetic content, it becomes possible to track the entire lifecycle of a deepfake—from creation to distribution. This traceability aids in pinpointing sources and accountability.

4. **Accountability and Control**: Technological measures provide a means of accountability by linking users and platforms to specific actions concerning deepfakes. This adds an additional layer of control, making it harder for individuals and companies to engage in harmful activities without facing consequences.

5. **Rapid Removal**: Technology facilitates the swift removal or disabling of access to malicious content, minimizing potential harm before deepfakes spread further.

6. **Collaboration Bridge**: Technological tools act as a bridge between legal frameworks and enforcement mechanisms, ensuring effective collaboration among stakeholders and enhancing overall enforcement capabilities.

In conclusion, while existing laws provide foundational protection against deepfakes, technological intervention significantly enhances the effectiveness of enforcement by providing tracking, automation, accountability, rapid removal, and fostering collaboration. This integrated approach is crucial for countering the evolving threat posed by malicious deepfakes effectively.

**Human Answer:** While the existing legal framework may be adequate for detecting, preventing, removing, and prosecuting the creation and distribution of malicious synthetic media, technological intervention is necessary for its effective enforcement. This is because technological measures can enable stakeholders to effectively comply with the law, and authorities to enforce it by detecting malicious deepfakes in time and/or removing them before they cause serious harm.

**Scores:** Similarity: 0.742, ROUGE-L: 0.123, BLEU: 0.009, METEOR: 0.343, BERT Precision: 0.823, BERT Recall: 0.881, BERT F1: 0.851

**Question:** How can traceability and watermarking help in tracking and preventing the spread of harmful AI-generated content?

**LLM Answer:** Traceability and watermarking are powerful tools for combating harmful AI-generated content like deepfakes, supported by existing legal frameworks such as the IT Act, IPC/BNS2, and intermediary guidelines. Here's how they can be effectively integrated:

1. **Traceability**: 
   - Assign unique, immutable IDs to content creators and platforms using technologies like blockchain. These IDs ensure each piece of deepfake content can be traced back to its source.
   - By linking these IDs through the AI's workflow (creation, distribution), it becomes easier to identify the origin and spread of harmful content.

2. **Watermarking**:
   - Embed unique identifiers into generated content at every stage using methods like digital signatures or blockchain. These watermarks are unchangeable, ensuring they remain intact even if content is altered.
   - This allows detection of tampering and tracks distribution attempts, aiding in rapid removal and accountability.

3. **Collective Data**:
   - Collect statistics on how deepfakes spread, informed by these IDs and watermarks. This data can improve AI models to reduce harmful outputs and develop better detection methods.

4. **Integration**:
   - Combine these techniques into content fingerprinting, embedding signatures that trace through all distribution channels. This ensures any harm is traced back to its source efficiently.

By implementing these strategies, platforms can effectively prevent the spread of harmful deepfakes while ensuring accountability and maintaining privacy standards.

**Human Answer:** Traceability and watermarking are suggested as technological measures that can help in tracking and preventing the spread of harmful AI-generated content1. Establishing unique and immutable identities for participants such as content creators, publishers, and social media platforms, and using these to watermark inputs and outputs of generative AI tools, could allow for tracking and analyzing the lifecycle of a deepfake from creation to use. This can help in determining when harmful content has been created and disseminated.

**Scores:** Similarity: 0.797, ROUGE-L: 0.192, BLEU: 0.031, METEOR: 0.312, BERT Precision: 0.829, BERT Recall: 0.895, BERT F1: 0.861


--------------------------------------------------------------------------------
