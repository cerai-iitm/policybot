# Context: 

The Digital Personal Data Protection Bill, 2023 was introduced in Lok Sabha on August 3, 2023. The Bill seeks to provide for the protection of personal data and the privacy of individuals. Applicability: The Bill applies to the processing of digital personal data within India where such data is: (i) collected online, or (ii) collected offline and is digitised. It will also apply to the processing of personal data outside India if it is for offering goods or services in India. Personal data is defined as any data about an individual who is identifiable by or in relation to such data. Processing has been defined as wholly or partially automated operation or set of operations performed on digital personal data. It includes collection, storage, use, and sharing. Consent: Personal data may be processed only for a lawful purpose after obtaining the consent of the individual. A notice must be given before seeking consent. The notice should contain details about the personal data to be collected and the purpose of processing. Consent may be withdrawn at any point in time. Consent will not be required for 'legitimate uses' including: (i) specified purpose for which data has been provided by an individual voluntarily, (ii) provision of benefit or service by the government, (iii) medical emergency, and (iv) employment. For individuals below 18 years of age, consent will be provided by the parent or the legal guardian. Rights and duties of data principal: An individual, whose data is being processed (data principal), will have the right to: (i) obtain information about processing, (ii) seek correction and erasure of personal data, (iii) nominate another person to exercise rights in the event of death or incapacity, and (iv) grievance redressal. Data principals will have certain duties. They must not: (i) register a false or frivolous complaint, and (ii) furnish any false particulars or impersonate another person in specified cases. Violation of duties will be punishable with a penalty of up to Rs 10,000. Obligations of data fiduciaries: The entity, determining the purpose and means of processing, (data fiduciary), must: (i) make reasonable efforts to ensure the accuracy and completeness of data, (ii) build reasonable security safeguards to prevent a data breach, (iii) inform the Data Protection Board of India and affected persons in the event of a breach, and (iv) erase personal data as soon as the purpose has been met and retention is not necessary for legal purposes (storage limitation). In case of government entities, storage limitation and the right of the data principal to erasure will not apply. Significant data fiduciaries: Certain data fiduciaries may be designated as significant data fiduciaries. Certain factors must be taken into regard such as: (i) volume and sensitivity of personal data processed, (ii) risks to the rights of data principals, (iii) security of the state, and (iv) public order. These entities will have certain additional obligations including: (i) appointing a data protection officer, and (ii) undertaking impact assessment and compliance audit. Exemptions: Rights of the data principal and obligations of data fiduciaries (except data security) will not apply in specified cases. These include: (i) prevention and investigation of offences, and (ii) enforcement of legal rights or claims. The central government may, by notification, exempt certain activities from the application of the Bill. These include: (i) processing by government entities in the interest of the security of the state and public order, and (ii) research, archiving, or statistical purposes. Processing of personal data of children: While processing the personal data of a child, the data fiduciary must not undertake: (i) processing that is likely to cause any detrimental effect on the well-being of the child, and (ii) tracking, behavioural monitoring, or targeted advertising. Cross-border transfer: The Bill allows the transfer of personal data outside India, except to countries restricted by the government through notification. Data Protection Board of India: The central government will establish the Data Protection Board of India. Key functions of the Board include: (i) monitoring compliance and imposing penalties, (ii) directing data fiduciaries to take necessary measures in the event of a data breach, and (iii) hearing grievances made by affected persons. Board members will be appointed for two years and will be eligible for re-appointment. Penalties: The schedule to the Bill specifies penalties for various offences such as up to: (i) Rs 200 crore for non-fulfilment of obligations for children, and (ii) Rs 250 crore for failure to take security measures to prevent data breaches.

**Question:** What are the key applicability criteria for the Digital Personal Data Protection Bill, 2023?

**LLM Answer:** The key applicability criteria for the Digital Personal Data Protection Bill, 2023 are:

1. **Digital personal data collection**: The Bill applies to the processing of digital personal data within India where such data is collected online or offline and digitized.
2. **Offering goods or services in India**: If personal data is processed outside India for offering goods or services in India, it will also be subject to the Bill's provisions.
3. **Lawful purpose and consent**: Personal data may only be processed for a lawful purpose after obtaining the individual's consent.

Additionally, certain activities are exempt from the applicability of the Bill, including:

1. **Prevention and investigation of offences**
2. **Enforcement of legal rights or claims**
3. **Research, archiving, or statistical purposes** (with government notification)

Note: The Bill also specifies that it does not apply to processing by government entities in the interest of the security of the state and public order, but this is an exemption through notification.

**Human Answer:** Applicability Criteria: The Bill applies to the processing of digital personal data within India if the data is (i) collected online, or (ii) collected offline and then digitized. It also applies to processing personal data outside India if it involves offering goods or services within India.

**Scores:** Similarity: 0.845, ROUGE-L: 0.301, BLEU: 0.095, METEOR: 0.425, BERT Precision: 0.843, BERT Recall: 0.915, BERT F1: 0.877

**Question:** Under what conditions can personal data be processed without the consent of the individual?

**LLM Answer:** Personal data may be processed without the consent of the individual in the following conditions:

1. **Specified purposes**: For specified purpose for which data has been provided by an individual voluntarily.
2. **Government provision of benefits or services**: For provision of benefit or service by the government.
3. **Medical emergency**: In a medical emergency, personal data may be processed without consent to provide necessary treatment or care.
4. **Employment**: Personal data may be processed in employment-related contexts without explicit consent.

Additionally, certain exceptions and exemptions are provided for:

1. **Prevention and investigation of offences**: Processing of personal data is exempted for the prevention and investigation of offences.
2. **Enforcement of legal rights or claims**: Processing of personal data is also exempted for enforcement of legal rights or claims.
3. **Government entities in national interest**: The central government may, by notification, exempt certain activities from the application of the Bill when processing by government entities is in the interest of the security of the state and public order.

It's worth noting that even in these cases where consent may not be required, data fiduciaries must still ensure they are processing personal data for a lawful purpose and following the principles outlined in the Bill.

**Human Answer:** Conditions for Processing Data Without Consent: Personal data can be processed without individual consent for 'legitimate uses,' including when data is voluntarily provided for a specified purpose, for the provision of government benefits or services, during medical emergencies, and for employment purposes.

**Scores:** Similarity: 0.849, ROUGE-L: 0.178, BLEU: 0.014, METEOR: 0.327, BERT Precision: 0.828, BERT Recall: 0.89, BERT F1: 0.858

**Question:** What rights are granted to data principals under the Bill?

**LLM Answer:** Under the Bill, the rights granted to data principals include:

1. The right to obtain information about processing.
2. The right to seek correction of personal data.
3. The right to erasure (deletion) of personal data.
4. The right to nominate another person to exercise their rights in case of death or incapacity.
5. The right to grievance redressal.

These rights are intended to empower individuals whose data is being processed, allowing them to have control over how their personal information is handled and protected.

**Human Answer:** Rights of Data Principals: Data principals, or individuals whose data is being processed, have the right to (i) obtain information about the processing of their data, (ii) seek correction and erasure of their personal data, (iii) nominate someone to exercise their rights if they die or become incapacitated, and (iv) grievance redressal.

**Scores:** Similarity: 0.877, ROUGE-L: 0.35, BLEU: 0.15, METEOR: 0.403, BERT Precision: 0.9, BERT Recall: 0.9, BERT F1: 0.9

**Question:** What obligations do data fiduciaries have in ensuring data protection and security?

**LLM Answer:** According to the provided context, data fiduciaries have several obligations in ensuring data protection and security, including:

1. **Making reasonable efforts to ensure accuracy and completeness of data**: Data fiduciaries must make sure that the data they collect is accurate and complete.
2. **Building reasonable security safeguards to prevent a data breach**: Data fiduciaries are responsible for creating measures to protect against unauthorized access or breaches of personal data.
3. **Informing the Data Protection Board of India and affected persons in the event of a breach**: If a data breach occurs, data fiduciaries must notify both the Data Protection Board of India and the individuals whose data was compromised.
4. **Erasing personal data as soon as the purpose has been met and retention is not necessary for legal purposes (storage limitation)**: Data fiduciaries are required to delete or erase personal data once its intended use is complete, unless it needs to be retained for specific legal reasons.

Data fiduciaries that are designated as significant may have additional obligations, such as:

1. **Appointing a data protection officer**: These entities must designate someone to oversee and manage their data protection efforts.
2. **Undertaking impact assessment and compliance audit**: Significant data fiduciaries must conduct regular assessments to ensure they are meeting the requirements of the Digital Personal Data Protection Bill.

These obligations aim to protect individuals' personal data and uphold their rights as outlined in the bill.

**Human Answer:** Obligations of Data Fiduciaries: Data fiduciaries, entities determining the purpose and means of data processing, must (i) make reasonable efforts to ensure data accuracy and completeness, (ii) build reasonable security safeguards to prevent data breaches, (iii) inform the Data Protection Board of India and affected individuals of data breaches, and (iv) erase personal data when its purpose has been met and retention is no longer legally necessary. Note that storage limitation and the right of the data principal to erasure do not apply to government entities.

**Scores:** Similarity: 0.928, ROUGE-L: 0.325, BLEU: 0.095, METEOR: 0.437, BERT Precision: 0.859, BERT Recall: 0.887, BERT F1: 0.873

**Question:** What are the penalties specified in the Bill for non-compliance with data protection obligations?

**LLM Answer:** According to the Bill, the penalties specified for non-compliance with data protection obligations include:

* Violation of duties by a data principal: Up to Rs 10,000.
* Non-fulfilment of obligations for children: Up to Rs 200 crore.
* Failure to take security measures to prevent data breaches: Up to Rs 250 crore.

Additionally, the Bill specifies penalties for other offences such as non-compliance with storage limitation and erasure rights, and failure to inform the Data Protection Board of India and affected persons in case of a data breach.

**Human Answer:** Penalties for Non-Compliance: The Bill specifies penalties for various offenses, including (i) up to Rs 200 crore for not fulfilling obligations related to children's data, and (ii) up to Rs 250 crore for failing to implement security measures to prevent data breaches

**Scores:** Similarity: 0.888, ROUGE-L: 0.331, BLEU: 0.17, METEOR: 0.507, BERT Precision: 0.879, BERT Recall: 0.913, BERT F1: 0.896


--------------------------------------------------------------------------------

# Context: 

The committee recommends the following: 1. To implement a whole-of-government approach to AI Governance, MeitY and the Principal Scientific Adviser should establish an empowered mechanism to coordinate AI Governance. The empowered mechanism should be in the form of an Inter-Ministerial AI Coordination Committee or Governance Group (Committee/ Group). It should bring together the various authorities and institutions that deal with AI Governance at the national level. The Committee/ Group should have an ongoing status and should not be a limited duration mechanism. The overall purpose of this Committee/ Group should be to bring the key institutions around a common roadmap and to coordinate their efforts to implement a whole-of government approach. A collaborative and co-ordinated approach by various regulators can enable them to be more efficient and effective, given the complexity likely to be involved in dealing with AI systems at scale, especially when we take a long-term view of the diffusion stage of their lifecycle. This can be especially necessary in domains and areas where multiple authorities may be concerned (e.g., consumer protection, food, transportation, agriculture, health care, etc.). The Committee/ Group should enable a whole of government approach to the AI ecosystem. Currently, regulators and government departments may have some visibility on the AI systems developed or deployed by entities who are under sectoral regulation (e.g., finance or health) or where the market is concentrated (e.g., ecommerce, social media, aggregators). However, the level of visibility would need to be adequate to assess potential risks associated with such entities in the context of AI. Further, there are likely to be AI systems developed or being developed and/ or deployed by entities who may not have an interface with the government/ regulators from a perspective of affording suitable visibility to enable a risk assessment in relation to AI. A pre-requisite of governance would be for the government and the regulators to have a credible understanding the AI ecosystem in the country so that governance measures are rooted to the realities of existing and likely risks. The Committee/ Group should facilitate this task. This would require a conversation-led approach with a view to develop an understanding of the ecosystem which can both serve as feedback for strengthening governance and enable understanding of possible challenges and gaps in complying Page 13 of 22 and enforcing existing laws. It is important to emphasise that such a mapping exercise should not result in regulatory overreach through at scale registration and reporting requirements. With the above context, the Committee/ Group should meet at a regular basis to suggest measures to catalyse collaboration between departments and regulators, so that they can: • apply and strengthen existing laws to minimise risk of harm due to use of AI; • provide legal clarity and certainty around development and use of AI by issuing joint guidance; • harmonise existing efforts and initiatives around common terminologies and risk inventories; • enable demonstrable self-regulation to operationalise the responsible AI principles; • take coordinated steps to respond to identified gaps with the benefit of multiregulatory support; • create a policy environment which enables the use of AI for beneficial use-cases; and • promote the development and deployment of responsible AI applications in their domains/sectors. In order to enable appropriate measurement of fairness, accountability and transparency in the Indian context, it is an essential prerequisite to have access to the right datasets, relevant to the Indian context, which allows users to assess the fairness and bias of their models across standard datasets. The creation of better datasets for the Indian context should be stimulated, and sector-specific datasets should be identified to enable creation and evaluation of fair models. These initiatives may be encouraged by the Committee/ Group. The Committee/ Group should have a mix of both official and non-official members, because such a forum focused on coordinating AI governance must also bring in external expertise from industry and academia, given their central role in operationalising responsible AI principles in practice. It may be headed by the Principal Scientific Adviser. Official members could include representatives deputed from MeitY, the NITI Aayog, the Telecommunication Engineering Centre, Bureau of Indian Standards, other departments of the Central Government, as well as sectoral regulators (e.g., RBI, Indian Council of Medical Research, SEBI, IRDAI, Telecom Regulatory Authority of India, etc.). Non-official members could include persons capable of representing the interests of AI developers, AI deployers, data providers, data principals, and end-users – so that the perspectives of the overall ecosystem can be considered. The Committee/ Group should invite external experts for discussions to understand and take on board diverse perspectives.

**Question:** What is the primary objective of the proposed Inter-Ministerial AI Coordination Committee or Governance Group?

**LLM Answer:** The primary objective of the proposed Inter-Ministerial AI Coordination Committee or Governance Group is to bring together various authorities and institutions dealing with AI governance at the national level, facilitating a collaborative and coordinated approach to implement a whole-of-government approach to AI. This will enable regulators to be more efficient and effective in addressing the complexity involved in dealing with AI systems, especially when taking a long-term view of their lifecycle.

**Human Answer:** Primary Objective: The primary objective of the proposed Inter-Ministerial AI Coordination Committee or Governance Group is to bring key institutions together around a common roadmap and to coordinate their efforts to implement a whole-of-government approach to AI governance1. This includes enabling a whole-of-government approach to the AI ecosystem.

**Scores:** Similarity: 0.94, ROUGE-L: 0.547, BLEU: 0.309, METEOR: 0.569, BERT Precision: 0.917, BERT Recall: 0.942, BERT F1: 0.93

**Question:** Why is a whole-of-government approach considered essential for AI governance in India?

**LLM Answer:** A whole-of-government approach is considered essential for AI governance in India because it enables a collaborative and coordinated effort by various regulators and institutions to implement policies, laws, and regulations effectively. This approach allows for the sharing of knowledge, resources, and expertise, which can help minimize risks associated with AI systems.

Key benefits include:

1.  **Efficient and effective regulation**: By working together, regulators can address complex issues more efficiently.
2.  **Better understanding of the AI ecosystem**: A whole-of-government approach facilitates a comprehensive understanding of the AI landscape in India, enabling informed decision-making and tailored governance measures.
3.  **Harmonization of efforts**: This approach helps harmonize existing initiatives, reducing duplication of effort and ensuring consistent application of laws and regulations.
4.  **Promotion of responsible AI**: By facilitating collaboration between departments and regulators, a whole-of-government approach encourages the development and deployment of responsible AI applications.

Overall, a whole-of-government approach is crucial for effective AI governance in India, as it enables coordinated efforts to address potential risks, promote responsible AI, and create a supportive policy environment.

**Human Answer:** Importance of a Whole-of-Government Approach: A whole-of-government approach is considered essential because a collaborative and coordinated approach by various regulators can enable them to be more efficient and effective, given the complexity likely to be involved in dealing with AI systems at scale, especially in the long term1. This is particularly necessary in areas where multiple authorities may be concerned, such as consumer protection, food, transportation, agriculture, and health care.

**Scores:** Similarity: 0.756, ROUGE-L: 0.227, BLEU: 0.07, METEOR: 0.35, BERT Precision: 0.854, BERT Recall: 0.883, BERT F1: 0.868

**Question:** What are the key functions and responsibilities of the AI Governance Committee/Group?

**LLM Answer:** The key functions and responsibilities of the AI Governance Committee/Group include:

1. **Coordinating a whole-of-government approach**: Bringing together various authorities and institutions dealing with AI governance at the national level.
2. **Implementing regulatory coordination**: Facilitating collaboration between departments, regulators, and stakeholders to ensure efficient and effective governance.
3. **Risk assessment and mapping**: Developing an understanding of the AI ecosystem in India to identify potential risks associated with AI systems.
4. **Issuing joint guidance and regulations**: Providing legal clarity and certainty around development and use of AI through joint guidelines.
5. **Harmonizing existing efforts**: Coordinating initiatives to align common terminologies, risk inventories, and regulatory approaches.
6. **Promoting self-regulation**: Encouraging demonstrable self-regulation to operationalize responsible AI principles.
7. **Addressing gaps and challenges**: Taking coordinated steps to respond to identified gaps with the benefit of multiregulatory support.
8. **Creating a favorable policy environment**: Enabling the use of AI for beneficial use-cases through policy interventions.
9. **Developing datasets for fairness assessment**: Stimulating creation of better datasets, relevant to the Indian context, for evaluating model fairness and bias.
10. **Ensuring diverse perspectives**: Including both official and non-official members with a mix of expertise from industry, academia, and government.

These functions aim to ensure responsible AI governance, transparency, accountability, and fairness in India's AI ecosystem.

**Human Answer:** Key Functions and Responsibilities: The AI Governance Committee/Group is responsible for: 1) Suggesting measures to catalyze collaboration between departments and regulators. 2) Applying and strengthening existing laws to minimize the risk of harm due to the use of AI. 3) Providing legal clarity and certainty around the development and use of AI by issuing joint guidance.4) Harmonizing existing efforts and initiatives around common terminologies and risk inventories. 5) Enabling demonstrable self-regulation to operationalize responsible AI principles. 6) Taking coordinated steps to respond to identified gaps with multi-regulatory support. 7) Creating a policy environment that enables the use of AI for beneficial use-cases. 8) Promoting the development and deployment of responsible AI applications in various domains and sectors.

**Scores:** Similarity: 0.896, ROUGE-L: 0.454, BLEU: 0.152, METEOR: 0.476, BERT Precision: 0.862, BERT Recall: 0.911, BERT F1: 0.886

**Question:** How does the committee propose to ensure fairness, accountability, and transparency in AI systems within the Indian context?

**LLM Answer:** To ensure fairness, accountability, and transparency in AI systems within the Indian context, the committee proposes a whole-of-government approach through an empowered Inter-Ministerial AI Coordination Committee or Governance Group (Committee/Group). This group will bring together various authorities and institutions to coordinate efforts, provide legal clarity, harmonize existing initiatives, enable demonstrable self-regulation, and promote the development of responsible AI applications. Additionally, the committee suggests stimulating the creation of better datasets relevant to the Indian context to allow for assessment of model fairness and bias. The Committee/Group will comprise both official and non-official members, including representatives from various departments and sectoral regulators, as well as external experts from industry and academia to ensure diverse perspectives are considered.

**Human Answer:** Ensuring Fairness, Accountability, and Transparency: The committee proposes to ensure fairness, accountability, and transparency in AI systems by: ◦ Promoting the creation of better datasets relevant to the Indian context, which allows users to assess the fairness and bias of their models across standard datasets. ◦ Identifying sector-specific datasets to enable the creation and evaluation of fair models.

**Scores:** Similarity: 0.82, ROUGE-L: 0.313, BLEU: 0.185, METEOR: 0.371, BERT Precision: 0.853, BERT Recall: 0.901, BERT F1: 0.876

**Question:** Who are the proposed members of the AI Governance Committee/Group, and why is it important to include both official and non-official members?

**LLM Answer:** The proposed members of the AI Governance Committee/Group include both official and non-official representatives. Official members would consist of deputed representatives from MeitY, NITI Aayog, Telecommunication Engineering Centre, Bureau of Indian Standards, other Central Government departments, and sectoral regulators (e.g., RBI, Indian Council of Medical Research, SEBI, IRDAI, Telecom Regulatory Authority of India). Non-official members would include experts representing AI developers, deployers, data providers, data principals, and end-users to ensure diverse perspectives are considered. The inclusion of both official and non-official members is crucial for effective governance, as it allows the Committee/Group to tap into external expertise from industry and academia to operationalize responsible AI principles in practice.

The importance of including both official and non-official members lies in their ability to provide a collaborative and co-ordinated approach to addressing complex AI-related issues. By bringing together diverse perspectives, the Committee/Group can create a whole-of-government approach to the AI ecosystem, ensure that governance measures are rooted in reality, and develop an understanding of the ecosystem that can serve as feedback for strengthening governance and compliance with existing laws.

**Human Answer:** Proposed Members: The AI Governance Committee/Group should have a mix of both official and non-official members. ◦Official members could include representatives from MeitY, NITI Aayog, the Telecommunication Engineering Centre, Bureau of Indian Standards, other central government departments, and sectoral regulators such as RBI, Indian Council of Medical Research, SEBI, IRDAI, and Telecom Regulatory Authority of India. ◦Non-official members could include individuals representing the interests of AI developers, AI deployers, data providers, data principals, and end-users. ◦It is important to include both official and non-official members to bring in external expertise from industry and academia, given their central role in operationalizing responsible AI principles in practice, and to consider the perspectives of the overall ecosystem1. The Committee/ Group should invite external experts for discussions to understand and take on board diverse perspectives.

**Scores:** Similarity: 0.915, ROUGE-L: 0.552, BLEU: 0.354, METEOR: 0.602, BERT Precision: 0.925, BERT Recall: 0.943, BERT F1: 0.934


--------------------------------------------------------------------------------

# Context: 

The need to enable effective compliance and enforcement of existing laws. 1. Deepfakes/ fakes/ malicious content There are existing legal safeguards/instruments to protect against misuse of foundation models for creating malicious synthetic media (i.e., malicious ‘deepfakes’). In this case, depending upon the context and negative effect of the malicious synthetic media in question, multiple laws can apply. For example: See: Rahul Matthan, The Zone of Mischief (2024) 8 Page 7 of 22 ● Information Technology Act, 2000 (IT Act): Section 66D of the IT Act criminalises the use of computer resources for cheating by personation. Section 66E prescribes the punishment for capturing and publishing or transmitting the image of a private area of any person without his or her consent. Publishing or transmitting obscene material for instance, which could be generated by using deepfake technology is a punishable offence under Section 67A and 67B of the IT Act. ● Indian Penal Code (IPC): In addition to the IT Act, certain harms/cybercrimes perpetuated by AI could also fall under the IPC. For instance, identity theft and cheating by personation are offences under Section 419 (cheating by personation), section 463 and 465 (forgery for the purpose of cheating), section 292 and 294 (selling/circulating/distributing obscene objects), and section 499 (causing harm to reputation/defamation). It is to be noted that the IPC has been recently replaced by the Bharatiya Nyaya (Second) Sanhita (BNS2), and the BNS2 retains these offences. ● Other laws: In addition to the IT Act and IPC / BNS2, there could be more laws depending on the nature of crime or cause of actions involved, like Prevention of Children from Sexual Offences Act, 2012 (section 12) in the event of sexual harassment of children, Juvenile Justice (Care and Protection of Children) Act, 2015 (section 75) for causing harm to the children, the Copyrights Act (section 51), if synthetic content infringes copyrighted work. It is worth noting that existing laws can also require specific measures from platforms and online service providers to detect and remove malicious synthetic media. Under the Information Technology (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021: ● Rule 3(1)(b) requires intermediaries to inform its rules and regulations, privacy policy and user agreement to a user and to make reasonable efforts to prevent the dissemination of specific types of content, that may cause harm to its users – including information that may violate bodily privacy, cause harm to a child, is deceptive, among other things. ● Rule 3(1)(c) requires intermediaries to periodically inform their users about the effects of non-compliance with the rules and regulations, privacy policy, or user agreement of such intermediary. ● Rule 3(2)(b) requires the intermediary to, within 24 hours of receiving user complaint of content which is ‘impersonation in electronic form, including artificially morphed images of such individual’ remove or disable its access. Further, the grievance officer appointed by the intermediary should acknowledge user complaints within 24 hours. The above shows that the legal framework may be adequate for the purposes of detecting, preventing, removing, and prosecuting the creation and distribution of malicious synthetic media. However, for this legal framework to be effective, it needs to be backed by requisite capabilities to enable stakeholders to effectively comply and for the authorities to enforce the legal framework. Page 8 of 22 This area points to possible gaps and opportunities for using technological measures for enabling effective compliance, so that malicious deepfakes are detected in time and/ or are removed before they cause serious harm. For example, as suggested above, traceability may be established by assigning unique and immutable identities to different participants, such as content creators, publishers, social media platforms, etc. These may then be used to watermark inputs to, and outputs from, generative AI tools. These may be used to track and analyse the lifecycle, from creation to use, of a deepfake – and to determine when they have been cre

**Question:** How do the Information Technology Act, 2000, and the Indian Penal Code (or its replacement, BNS2) address cybercrimes related to deepfakes?

**LLM Answer:** The Information Technology Act, 2000 (IT Act) and the Indian Penal Code (IPC), which has been replaced by the Bharatiya Nyaya (Second) Sanhita (BNS2), address cybercrimes related to deepfakes in various ways. The IT Act specifically criminalizes the use of computer resources for cheating by personation, capturing and publishing or transmitting the image of a private area without consent, and publishing or transmitting obscene material generated using deepfake technology. The IPC retains similar offenses under sections 419 (cheating by personation), section 463 and 465 (forgery for the purpose of cheating), section 292 and 294 (selling/circulating/distributing obscene objects), and section 499 (causing harm to reputation/defamation).

**Human Answer:** The Information Technology Act, 2000 (IT Act), addresses cybercrimes related to deepfakes through several sections:• Section 66D criminalizes the use of computer resources for cheating by personation. This could apply if a deepfake is used to impersonate someone for fraudulent purposes.• Section 66E prescribes punishment for capturing and publishing or transmitting the image of a private area of any person without their consent. Deepfakes creating such images could fall under this section.• Sections 67A and 67B make publishing or transmitting obscene material punishable offences1. This could include obscene content generated using deepfake technology. The Indian Penal Code (IPC) and its recent replacement, the Bharatiya Nyaya (Second) Sanhita (BNS2), also address harms and cybercrimes that could be perpetrated by AI-generated malicious synthetic media: • Section 419 (cheating by personation) of the IPC criminalizes identity theft and cheating by personation1. This offense is retained in the BNS2. •Sections 463 and 465 (forgery for the purpose of cheating) of the IPC address forgery with the intent to deceive1. These offences are also retained in the BNS2. • Sections 292 and 294 (selling/circulating/distributing obscene objects) of the IPC criminalize the dissemination of obscene material. These offences are retained in the BNS2. • Section 499 (causing harm to reputation/defamation) of the IPC addresses defamation1. This offence is also retained in the BNS2. •In addition to these, other laws like the Prevention of Children from Sexual Offences Act, 2012 (section 12), can apply in cases of sexual harassment of children, and the Juvenile Justice (Care and Protection of Children) Act, 2015 (section 75), for causing harm to children1. The Copyrights Act (section 51) may also be relevant if synthetic content infringes copyrighted work

**Scores:** Similarity: 0.922, ROUGE-L: 0.405, BLEU: 0.145, METEOR: 0.284, BERT Precision: 0.932, BERT Recall: 0.864, BERT F1: 0.897

**Question:** What obligations do online intermediaries have under the IT (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021, in detecting and removing malicious synthetic media?

**LLM Answer:** Online intermediaries have several obligations under the IT (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021, in detecting and removing malicious synthetic media. These include:

1. Informing users about their rules and regulations, privacy policy, and user agreement.
2. Making reasonable efforts to prevent the dissemination of content that may cause harm to its users, including information that may violate bodily privacy or cause harm to a child.
3. Periodically informing users about the effects of non-compliance with these rules and regulations.
4. Removing or disabling access to content that is "impersonation in electronic form, including artificially morphed images of such individual" within 24 hours of receiving a user complaint.

These obligations are outlined in Rule 3(1)(b), Rule 3(1)(c), and Rule 3(2)(b) of the IT (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021.

**Human Answer:** Under the Information Technology (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021, online intermediaries have specific obligations in detecting and removing malicious synthetic media: •Rule 3(1)(b) requires intermediaries to inform users of their rules, regulations, privacy policy, and user agreement and to make reasonable efforts to prevent the dissemination of specific types of harmful content. This includes information that may violate bodily privacy, cause harm to a child, or is deceptive. •Rule 3(1)(c) mandates that intermediaries periodically inform their users about the consequences of non-compliance with their rules, regulations, privacy policy, or user agreement. •Rule 3(2)(b) requires intermediaries to remove or disable access to content within 24 hours of receiving a user complaint regarding 'impersonation in electronic form, including artificially morphed images of such individual'. Furthermore, the grievance officer appointed by the intermediary must acknowledge user complaints within 24 hour.

**Scores:** Similarity: 0.942, ROUGE-L: 0.566, BLEU: 0.447, METEOR: 0.575, BERT Precision: 0.928, BERT Recall: 0.924, BERT F1: 0.926

**Question:** Why is technological intervention necessary for the effective enforcement of laws against malicious deepfakes?

**LLM Answer:** Technological intervention is necessary for the effective enforcement of laws against malicious deepfakes because existing legal safeguards are inadequate without the ability to detect and track the origin and distribution of such content. Laws like the Information Technology Act, Indian Penal Code, and other relevant acts provide a framework for punishing creators and distributors of malicious synthetic media, but they rely on human reporting and investigation, which can be slow and ineffective.

The use of technological measures such as watermarking inputs to and outputs from generative AI tools, assigning unique and immutable identities to participants, and tracking the lifecycle of deepfakes can help bridge this gap. These technologies can enable stakeholders to effectively comply with laws and regulations, and authorities to enforce them, thereby preventing malicious deepfakes from causing harm.

In essence, technological intervention is necessary to supplement existing legal frameworks and ensure that malicious deepfakes are detected, removed, and prosecuted in a timely manner.

**Human Answer:** While the existing legal framework may be adequate for detecting, preventing, removing, and prosecuting the creation and distribution of malicious synthetic media, technological intervention is necessary for its effective enforcement. This is because technological measures can enable stakeholders to effectively comply with the law, and authorities to enforce it by detecting malicious deepfakes in time and/or removing them before they cause serious harm.

**Scores:** Similarity: 0.831, ROUGE-L: 0.292, BLEU: 0.132, METEOR: 0.512, BERT Precision: 0.875, BERT Recall: 0.916, BERT F1: 0.895

**Question:** How can traceability and watermarking help in tracking and preventing the spread of harmful AI-generated content?

**LLM Answer:** Traceability and watermarking can help in tracking and preventing the spread of harmful AI-generated content by enabling the identification and analysis of the content's lifecycle, from creation to dissemination. This can be achieved through assigning unique and immutable identities to different participants involved in creating or sharing the content. The use of watermarking can further aid in identifying the source and purpose of the synthetic media.

**Human Answer:** Traceability and watermarking are suggested as technological measures that can help in tracking and preventing the spread of harmful AI-generated content1. Establishing unique and immutable identities for participants such as content creators, publishers, and social media platforms, and using these to watermark inputs and outputs of generative AI tools, could allow for tracking and analyzing the lifecycle of a deepfake from creation to use. This can help in determining when harmful content has been created and disseminated.

**Scores:** Similarity: 0.846, ROUGE-L: 0.384, BLEU: 0.211, METEOR: 0.301, BERT Precision: 0.929, BERT Recall: 0.912, BERT F1: 0.921


--------------------------------------------------------------------------------
