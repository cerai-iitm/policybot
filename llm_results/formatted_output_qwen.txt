# Context: 

The Digital Personal Data Protection Bill, 2023 was introduced in Lok Sabha on August 3, 2023. The Bill seeks to provide for the protection of personal data and the privacy of individuals. Applicability: The Bill applies to the processing of digital personal data within India where such data is: (i) collected online, or (ii) collected offline and is digitised. It will also apply to the processing of personal data outside India if it is for offering goods or services in India. Personal data is defined as any data about an individual who is identifiable by or in relation to such data. Processing has been defined as wholly or partially automated operation or set of operations performed on digital personal data. It includes collection, storage, use, and sharing. Consent: Personal data may be processed only for a lawful purpose after obtaining the consent of the individual. A notice must be given before seeking consent. The notice should contain details about the personal data to be collected and the purpose of processing. Consent may be withdrawn at any point in time. Consent will not be required for 'legitimate uses' including: (i) specified purpose for which data has been provided by an individual voluntarily, (ii) provision of benefit or service by the government, (iii) medical emergency, and (iv) employment. For individuals below 18 years of age, consent will be provided by the parent or the legal guardian. Rights and duties of data principal: An individual, whose data is being processed (data principal), will have the right to: (i) obtain information about processing, (ii) seek correction and erasure of personal data, (iii) nominate another person to exercise rights in the event of death or incapacity, and (iv) grievance redressal. Data principals will have certain duties. They must not: (i) register a false or frivolous complaint, and (ii) furnish any false particulars or impersonate another person in specified cases. Violation of duties will be punishable with a penalty of up to Rs 10,000. Obligations of data fiduciaries: The entity, determining the purpose and means of processing, (data fiduciary), must: (i) make reasonable efforts to ensure the accuracy and completeness of data, (ii) build reasonable security safeguards to prevent a data breach, (iii) inform the Data Protection Board of India and affected persons in the event of a breach, and (iv) erase personal data as soon as the purpose has been met and retention is not necessary for legal purposes (storage limitation). In case of government entities, storage limitation and the right of the data principal to erasure will not apply. Significant data fiduciaries: Certain data fiduciaries may be designated as significant data fiduciaries. Certain factors must be taken into regard such as: (i) volume and sensitivity of personal data processed, (ii) risks to the rights of data principals, (iii) security of the state, and (iv) public order. These entities will have certain additional obligations including: (i) appointing a data protection officer, and (ii) undertaking impact assessment and compliance audit. Exemptions: Rights of the data principal and obligations of data fiduciaries (except data security) will not apply in specified cases. These include: (i) prevention and investigation of offences, and (ii) enforcement of legal rights or claims. The central government may, by notification, exempt certain activities from the application of the Bill. These include: (i) processing by government entities in the interest of the security of the state and public order, and (ii) research, archiving, or statistical purposes. Processing of personal data of children: While processing the personal data of a child, the data fiduciary must not undertake: (i) processing that is likely to cause any detrimental effect on the well-being of the child, and (ii) tracking, behavioural monitoring, or targeted advertising. Cross-border transfer: The Bill allows the transfer of personal data outside India, except to countries restricted by the government through notification. Data Protection Board of India: The central government will establish the Data Protection Board of India. Key functions of the Board include: (i) monitoring compliance and imposing penalties, (ii) directing data fiduciaries to take necessary measures in the event of a data breach, and (iii) hearing grievances made by affected persons. Board members will be appointed for two years and will be eligible for re-appointment. Penalties: The schedule to the Bill specifies penalties for various offences such as up to: (i) Rs 200 crore for non-fulfilment of obligations for children, and (ii) Rs 250 crore for failure to take security measures to prevent data breaches.

**Question:** What are the key applicability criteria for the Digital Personal Data Protection Bill, 2023?

**LLM Answer:** The key applicability criteria for the Digital Personal Data Protection Bill, 2023 include:

1. **Processing Location**: The processing of digital personal data must occur within India if it is collected online or offline and subsequently digitized.
   
2. **Business Presence in India**: If the processing of personal data takes place outside India but is related to offering goods or services in India, the Bill applies.

3. **Definition of Personal Data**: Any data about an individual who can be identified by such data falls under this category.

4. **Processing Activities**: The Bill covers any operation performed on digital personal data, including collection, storage, use, and sharing.

5. **Consent Requirements**: Processing can only occur with consent from the individual or for 'legitimate uses' as specified in the Bill. For individuals below 18 years old, parental or guardian consent is required.

6. **Data Fiduciary Obligations**: Data fiduciaries must ensure data accuracy and completeness, build security safeguards, inform the Data Protection Board of India about breaches, and erase personal data when no longer needed for legal purposes.

7. **Significant Data Fiduciaries**: Certain entities may be designated as significant due to the volume or sensitivity of data processed, risks to individuals' rights, national security concerns, or public order issues. These entities must appoint a data protection officer and undertake impact assessments and compliance audits.

8. **Exemptions**: The Bill provides exemptions in certain cases such as prevention and investigation of offenses, enforcement of legal rights or claims, processing by government entities for security and public order reasons, and research, archiving, or statistical purposes.

**Human Answer:** Applicability Criteria: The Bill applies to the processing of digital personal data within India if the data is (i) collected online, or (ii) collected offline and then digitized. It also applies to processing personal data outside India if it involves offering goods or services within India.

**Scores:** Similarity: 0.822, ROUGE-L: 0.199, BLEU: 0.042, METEOR: 0.335, BERT Precision: 0.812, BERT Recall: 0.906, BERT F1: 0.856

**Question:** Under what conditions can personal data be processed without the consent of the individual?

**LLM Answer:** Personal data can be processed without the consent of the individual under the following conditions as per the Digital Personal Data Protection Bill, 2023:

1. **Specified Purpose**: If the personal data was provided by the individual voluntarily for a specific purpose.
2. **Government Services**: For providing benefits or services by the government.
3. **Medical Emergencies**: In cases where medical care is necessary and time-sensitive.
4. **Employment**: During the recruitment, employment, and termination processes.

Additionally, it's worth noting that these exceptions may also apply to individuals below 18 years of age, with consent required from their parent or legal guardian in such scenarios.

**Human Answer:** Conditions for Processing Data Without Consent: Personal data can be processed without individual consent for 'legitimate uses,' including when data is voluntarily provided for a specified purpose, for the provision of government benefits or services, during medical emergencies, and for employment purposes.

**Scores:** Similarity: 0.874, ROUGE-L: 0.274, BLEU: 0.065, METEOR: 0.417, BERT Precision: 0.843, BERT Recall: 0.899, BERT F1: 0.871

**Question:** What rights are granted to data principals under the Bill?

**LLM Answer:** Data principals are granted the following rights under the Digital Personal Data Protection Bill, 2023:

1. The right to obtain information about processing.
2. The right to seek correction and erasure of personal data.
3. The right to nominate another person to exercise their rights in the event of death or incapacity.
4. The right to grievance redressal.

These rights aim to protect the privacy and personal data of individuals within India, ensuring they have control over how their data is processed.

**Human Answer:** Rights of Data Principals: Data principals, or individuals whose data is being processed, have the right to (i) obtain information about the processing of their data, (ii) seek correction and erasure of their personal data, (iii) nominate someone to exercise their rights if they die or become incapacitated, and (iv) grievance redressal.

**Scores:** Similarity: 0.777, ROUGE-L: 0.373, BLEU: 0.129, METEOR: 0.453, BERT Precision: 0.893, BERT Recall: 0.9, BERT F1: 0.896

**Question:** What obligations do data fiduciaries have in ensuring data protection and security?

**LLM Answer:** Data fiduciaries have several key obligations under the Digital Personal Data Protection Bill, 2023 to ensure data protection and security:

1. **Ensure Accuracy and Completeness**: Make reasonable efforts to ensure that the personal data being processed is accurate and complete.
2. **Build Security Safeguards**: Develop reasonable security safeguards to prevent data breaches.
3. **Inform in Case of Breach**: Inform both the Data Protection Board of India and affected persons if a breach occurs.
4. **Erase Personal Data**: Erase personal data as soon as the purpose has been met, unless retention is necessary for legal reasons.

For significant data fiduciaries, additional obligations include:

1. **Appoint a Data Protection Officer**: Designate an officer to handle data protection matters.
2. **Undertake Impact Assessment and Compliance Audit**: Conduct regular assessments and audits to ensure compliance with the Bill's requirements.

These measures are crucial for safeguarding personal data and maintaining public trust in data processing activities.

**Human Answer:** Obligations of Data Fiduciaries: Data fiduciaries, entities determining the purpose and means of data processing, must (i) make reasonable efforts to ensure data accuracy and completeness, (ii) build reasonable security safeguards to prevent data breaches, (iii) inform the Data Protection Board of India and affected individuals of data breaches, and (iv) erase personal data when its purpose has been met and retention is no longer legally necessary. Note that storage limitation and the right of the data principal to erasure do not apply to government entities.

**Scores:** Similarity: 0.887, ROUGE-L: 0.361, BLEU: 0.131, METEOR: 0.461, BERT Precision: 0.86, BERT Recall: 0.884, BERT F1: 0.872

**Question:** What are the penalties specified in the Bill for non-compliance with data protection obligations?

**LLM Answer:** The Digital Personal Data Protection Bill, 2023 specifies penalties for non-compliance with data protection obligations as follows:

- For non-fulfillment of obligations towards children, the penalty is up to Rs 200 crore.
- For failure to take security measures to prevent data breaches, the penalty is up to Rs 250 crore.

These penalties are specified in the schedule to the Bill.

**Human Answer:** Penalties for Non-Compliance: The Bill specifies penalties for various offenses, including (i) up to Rs 200 crore for not fulfilling obligations related to children's data, and (ii) up to Rs 250 crore for failing to implement security measures to prevent data breaches

**Scores:** Similarity: 0.86, ROUGE-L: 0.4, BLEU: 0.222, METEOR: 0.529, BERT Precision: 0.899, BERT Recall: 0.916, BERT F1: 0.908


--------------------------------------------------------------------------------

# Context: 

The committee recommends the following: 1. To implement a whole-of-government approach to AI Governance, MeitY and the Principal Scientific Adviser should establish an empowered mechanism to coordinate AI Governance. The empowered mechanism should be in the form of an Inter-Ministerial AI Coordination Committee or Governance Group (Committee/ Group). It should bring together the various authorities and institutions that deal with AI Governance at the national level. The Committee/ Group should have an ongoing status and should not be a limited duration mechanism. The overall purpose of this Committee/ Group should be to bring the key institutions around a common roadmap and to coordinate their efforts to implement a whole-of government approach. A collaborative and co-ordinated approach by various regulators can enable them to be more efficient and effective, given the complexity likely to be involved in dealing with AI systems at scale, especially when we take a long-term view of the diffusion stage of their lifecycle. This can be especially necessary in domains and areas where multiple authorities may be concerned (e.g., consumer protection, food, transportation, agriculture, health care, etc.). The Committee/ Group should enable a whole of government approach to the AI ecosystem. Currently, regulators and government departments may have some visibility on the AI systems developed or deployed by entities who are under sectoral regulation (e.g., finance or health) or where the market is concentrated (e.g., ecommerce, social media, aggregators). However, the level of visibility would need to be adequate to assess potential risks associated with such entities in the context of AI. Further, there are likely to be AI systems developed or being developed and/ or deployed by entities who may not have an interface with the government/ regulators from a perspective of affording suitable visibility to enable a risk assessment in relation to AI. A pre-requisite of governance would be for the government and the regulators to have a credible understanding the AI ecosystem in the country so that governance measures are rooted to the realities of existing and likely risks. The Committee/ Group should facilitate this task. This would require a conversation-led approach with a view to develop an understanding of the ecosystem which can both serve as feedback for strengthening governance and enable understanding of possible challenges and gaps in complying Page 13 of 22 and enforcing existing laws. It is important to emphasise that such a mapping exercise should not result in regulatory overreach through at scale registration and reporting requirements. With the above context, the Committee/ Group should meet at a regular basis to suggest measures to catalyse collaboration between departments and regulators, so that they can: • apply and strengthen existing laws to minimise risk of harm due to use of AI; • provide legal clarity and certainty around development and use of AI by issuing joint guidance; • harmonise existing efforts and initiatives around common terminologies and risk inventories; • enable demonstrable self-regulation to operationalise the responsible AI principles; • take coordinated steps to respond to identified gaps with the benefit of multiregulatory support; • create a policy environment which enables the use of AI for beneficial use-cases; and • promote the development and deployment of responsible AI applications in their domains/sectors. In order to enable appropriate measurement of fairness, accountability and transparency in the Indian context, it is an essential prerequisite to have access to the right datasets, relevant to the Indian context, which allows users to assess the fairness and bias of their models across standard datasets. The creation of better datasets for the Indian context should be stimulated, and sector-specific datasets should be identified to enable creation and evaluation of fair models. These initiatives may be encouraged by the Committee/ Group. The Committee/ Group should have a mix of both official and non-official members, because such a forum focused on coordinating AI governance must also bring in external expertise from industry and academia, given their central role in operationalising responsible AI principles in practice. It may be headed by the Principal Scientific Adviser. Official members could include representatives deputed from MeitY, the NITI Aayog, the Telecommunication Engineering Centre, Bureau of Indian Standards, other departments of the Central Government, as well as sectoral regulators (e.g., RBI, Indian Council of Medical Research, SEBI, IRDAI, Telecom Regulatory Authority of India, etc.). Non-official members could include persons capable of representing the interests of AI developers, AI deployers, data providers, data principals, and end-users – so that the perspectives of the overall ecosystem can be considered. The Committee/ Group should invite external experts for discussions to understand and take on board diverse perspectives.

**Question:** What is the primary objective of the proposed Inter-Ministerial AI Coordination Committee or Governance Group?

**LLM Answer:** The primary objective of the proposed Inter-Ministerial AI Coordination Committee or Governance Group is to implement a whole-of-government approach to AI governance. Specifically, this involves:

1. Bringing together various authorities and institutions that deal with AI governance at the national level.
2. Ensuring ongoing coordination among these entities.
3. Facilitating collaboration between departments and regulators to apply, strengthen, and harmonize existing laws related to AI.
4. Providing legal clarity and certainty around AI development and use through joint guidance.
5. Enabling self-regulation based on responsible AI principles.
6. Taking coordinated steps to address identified gaps in governance with the support of multiple regulatory bodies.
7. Creating a policy environment that encourages beneficial uses of AI.
8. Promoting the development and deployment of responsible AI applications across different sectors.

In summary, the core aim is to coordinate efforts among various government entities to effectively manage and govern AI systems comprehensively, ensuring both safety and beneficial use.

**Human Answer:** Primary Objective: The primary objective of the proposed Inter-Ministerial AI Coordination Committee or Governance Group is to bring key institutions together around a common roadmap and to coordinate their efforts to implement a whole-of-government approach to AI governance1. This includes enabling a whole-of-government approach to the AI ecosystem.

**Scores:** Similarity: 0.872, ROUGE-L: 0.292, BLEU: 0.129, METEOR: 0.422, BERT Precision: 0.862, BERT Recall: 0.933, BERT F1: 0.896

**Question:** Why is a whole-of-government approach considered essential for AI governance in India?

**LLM Answer:** A whole-of-government approach is essential for AI governance in India because it ensures coordinated efforts across various authorities and institutions dealing with AI, enabling a more efficient and effective implementation of policies. This approach facilitates the harmonization of existing laws, initiatives, and terminologies related to AI, which is crucial given the complexity involved in managing AI systems at scale.

By bringing together key institutions such as MeitY, NITI Aayog, sectoral regulators (e.g., RBI, Indian Council of Medical Research), and others, an Inter-Ministerial AI Coordination Committee or Governance Group can provide a common roadmap for governance. This ensures that all relevant stakeholders are aligned and working towards shared objectives.

The committee's role in facilitating understanding of the AI ecosystem is also vital, as it helps address potential risks associated with AI systems deployed by various entities across different sectors. Regular meetings to suggest measures for collaboration between departments and regulators will enable them to apply existing laws effectively, provide legal clarity, harmonize efforts, encourage self-regulation, and respond to identified gaps.

Moreover, a whole-of-government approach fosters the development of appropriate datasets relevant to the Indian context, which is essential for ensuring fairness, accountability, and transparency in AI applications. By including external expertise from industry and academia, this committee can ensure that diverse perspectives are considered, leading to more responsible and beneficial use of AI technologies.

**Human Answer:** Importance of a Whole-of-Government Approach: A whole-of-government approach is considered essential because a collaborative and coordinated approach by various regulators can enable them to be more efficient and effective, given the complexity likely to be involved in dealing with AI systems at scale, especially in the long term1. This is particularly necessary in areas where multiple authorities may be concerned, such as consumer protection, food, transportation, agriculture, and health care.

**Scores:** Similarity: 0.642, ROUGE-L: 0.19, BLEU: 0.056, METEOR: 0.365, BERT Precision: 0.847, BERT Recall: 0.896, BERT F1: 0.871

**Question:** What are the key functions and responsibilities of the AI Governance Committee/Group?

**LLM Answer:** The key functions and responsibilities of the AI Governance Committee/Group include:

1. **Implementing a Whole-of-Government Approach**: Facilitating coordination among various authorities and institutions dealing with AI governance at the national level.
2. **Ensuring Ongoing Coordination**: Maintaining an ongoing status to coordinate efforts across different sectors and domains (e.g., consumer protection, food, transportation).
3. **Developing a Common Roadmap**: Aligning key institutions towards a common vision for AI governance.
4. **Facilitating Regulatory Visibility**: Ensuring adequate visibility into AI systems developed or deployed by entities under sectoral regulation and those without direct government interface to enable risk assessment.
5. **Understanding the AI Ecosystem**: Conducting mapping exercises and fostering conversations to develop an understanding of the AI ecosystem, ensuring that governance measures are practical and grounded in reality.
6. **Strengthening Existing Laws**: Applying and strengthening existing laws to minimize risks associated with AI use.
7. **Issuing Joint Guidance**: Providing legal clarity through joint guidance on AI development and use.
8. **Harmonizing Efforts**: Aligning efforts across different departments and initiatives by harmonizing terminologies and risk inventories.
9. **Promoting Responsible Self-Regulation**: Encouraging the operationalization of responsible AI principles, including self-regulatory measures.
10. **Responding to Gaps**: Addressing identified gaps in governance with multiregulatory support.
11. **Creating a Policy Environment**: Establishing policies that enable beneficial use-cases for AI.
12. **Promoting Responsible Applications**: Fostering the development and deployment of responsible AI applications across various sectors.

The Committee/Group should also:

- Encourage the creation of relevant datasets to measure fairness, accountability, and transparency in the Indian context.
- Include a mix of official and non-official members from diverse backgrounds (industry, academia) to ensure a comprehensive view of the AI ecosystem.

**Human Answer:** Key Functions and Responsibilities: The AI Governance Committee/Group is responsible for: 1) Suggesting measures to catalyze collaboration between departments and regulators. 2) Applying and strengthening existing laws to minimize the risk of harm due to the use of AI. 3) Providing legal clarity and certainty around the development and use of AI by issuing joint guidance.4) Harmonizing existing efforts and initiatives around common terminologies and risk inventories. 5) Enabling demonstrable self-regulation to operationalize responsible AI principles. 6) Taking coordinated steps to respond to identified gaps with multi-regulatory support. 7) Creating a policy environment that enables the use of AI for beneficial use-cases. 8) Promoting the development and deployment of responsible AI applications in various domains and sectors.

**Scores:** Similarity: 0.883, ROUGE-L: 0.341, BLEU: 0.097, METEOR: 0.438, BERT Precision: 0.847, BERT Recall: 0.902, BERT F1: 0.874

**Question:** How does the committee propose to ensure fairness, accountability, and transparency in AI systems within the Indian context?

**LLM Answer:** The committee proposes ensuring fairness, accountability, and transparency in AI systems within the Indian context through a multi-faceted approach:

1. **Whole-of-Government Approach**: The establishment of an Inter-Ministerial AI Coordination Committee or Governance Group (Committee/Group) to coordinate efforts across various authorities and institutions at the national level. This group would ensure collaboration and co-ordination, enhancing efficiency and effectiveness in managing complex AI systems.

2. **Visibility and Risk Assessment**: Ensuring adequate visibility into AI systems developed by entities under sectoral regulation and those with no direct interface with government/regulators to enable risk assessment and compliance with existing laws. The committee aims to develop a credible understanding of the AI ecosystem, which will inform governance measures tailored to Indian realities.

3. **Regular Meetings and Collaboration**: Regular meetings of the Committee/Group to suggest measures for collaboration between departments and regulators, aimed at applying and strengthening existing laws, providing legal clarity through joint guidance, harmonizing efforts, enabling self-regulation, and responding to gaps with multiregulatory support.

4. **Fairness and Bias Mitigation**: Encouraging the creation of relevant datasets that reflect the Indian context for assessing fairness and bias in models. The committee will promote initiatives that stimulate the development of sector-specific datasets necessary for creating and evaluating fair AI applications.

5. **Inclusive Membership and External Expertise**: A diverse membership comprising both official and non-official members, with external experts representing various stakeholders including developers, deployers, data providers, principals, and end-users to ensure a holistic view is considered in policy-making.

6. **Policy Environment for Beneficial Use-Cases**: Creating a policy environment that facilitates the use of AI for beneficial purposes while promoting responsible AI applications across different sectors.

By implementing these measures, the committee aims to foster fairness, accountability, and transparency in AI systems within the Indian context.

**Human Answer:** Ensuring Fairness, Accountability, and Transparency: The committee proposes to ensure fairness, accountability, and transparency in AI systems by: ◦ Promoting the creation of better datasets relevant to the Indian context, which allows users to assess the fairness and bias of their models across standard datasets. ◦ Identifying sector-specific datasets to enable the creation and evaluation of fair models.

**Scores:** Similarity: 0.789, ROUGE-L: 0.184, BLEU: 0.063, METEOR: 0.364, BERT Precision: 0.819, BERT Recall: 0.889, BERT F1: 0.853

**Question:** Who are the proposed members of the AI Governance Committee/Group, and why is it important to include both official and non-official members?

**LLM Answer:** The proposed members of the AI Governance Committee/Group include a mix of both official and non-official members.

### Official Members:
1. **MeitY (Ministry of Electronics and Information Technology)** - Representatives deputed from MeitY, which is directly involved in policy-making for digital technology.
2. **NITI Aayog** - The National Institution for Transforming India's think tank, which plays a crucial role in national development policies.
3. **Telecommunication Engineering Centre (TEC) and Bureau of Indian Standards (BIS)** - These organizations are responsible for setting standards and ensuring technical compliance, making them key players.
4. **Other Departments of the Central Government** - Various other ministries and departments that deal with different sectors like finance, health, agriculture, etc., which have a stake in AI governance.
5. **Sectoral Regulators** - Specific regulators such as:
   - **RBI (Reserve Bank of India)** for financial services,
   - **Indian Council of Medical Research (ICMR) and SEBI (Securities and Exchange Board of India)** for healthcare and finance sectors respectively,
   - **IRDAI (Insurance Regulatory and Development Authority of India)**
   - **Telecom Regulatory Authority of India (TRAI)**, which oversees telecommunications.

### Non-Official Members:
1. **Industry Representatives** - Individuals or organizations from the private sector involved in AI development and deployment.
2. **Academic Experts** - Researchers and professors from universities with expertise in AI and related fields.
3. **Data Providers** - Organizations that deal with large datasets essential for AI models.
4. **End-Users and Consumers** - Representatives who can provide insights into user perspectives, needs, and concerns.

### Importance of Including Both Official and Non-Official Members:
1. **Diverse Expertise**: Bringing together a mix of official and non-official members ensures that the committee has a broad spectrum of expertise covering both policy-making and practical implementation.
2. **Stakeholder Engagement**: Involving representatives from various sectors helps in understanding and addressing the diverse needs and challenges faced by different stakeholders, ensuring comprehensive governance.
3. **Transparency and Accountability**: Collaboration between official bodies and external experts enhances transparency and accountability, making the governance framework more robust.
4. **Innovation and Practicality**: External expertise can bring practical insights into the operationalization of responsible AI principles, ensuring that policies are grounded in real-world applications.
5. **Balanced Decision-Making**: The inclusion of diverse perspectives ensures balanced decision-making, reducing the risk of overreach or neglecting critical aspects of AI governance.

This mixed membership structure is crucial for fostering a collaborative and effective approach to AI governance, enabling a comprehensive understanding of the AI ecosystem and its potential impacts across various sectors in India.

**Human Answer:** Proposed Members: The AI Governance Committee/Group should have a mix of both official and non-official members. ◦Official members could include representatives from MeitY, NITI Aayog, the Telecommunication Engineering Centre, Bureau of Indian Standards, other central government departments, and sectoral regulators such as RBI, Indian Council of Medical Research, SEBI, IRDAI, and Telecom Regulatory Authority of India. ◦Non-official members could include individuals representing the interests of AI developers, AI deployers, data providers, data principals, and end-users. ◦It is important to include both official and non-official members to bring in external expertise from industry and academia, given their central role in operationalizing responsible AI principles in practice, and to consider the perspectives of the overall ecosystem1. The Committee/ Group should invite external experts for discussions to understand and take on board diverse perspectives.

**Scores:** Similarity: 0.829, ROUGE-L: 0.33, BLEU: 0.068, METEOR: 0.417, BERT Precision: 0.825, BERT Recall: 0.878, BERT F1: 0.851


--------------------------------------------------------------------------------

# Context: 

The need to enable effective compliance and enforcement of existing laws. 1. Deepfakes/ fakes/ malicious content There are existing legal safeguards/instruments to protect against misuse of foundation models for creating malicious synthetic media (i.e., malicious ‘deepfakes’). In this case, depending upon the context and negative effect of the malicious synthetic media in question, multiple laws can apply. For example: See: Rahul Matthan, The Zone of Mischief (2024) 8 Page 7 of 22 ● Information Technology Act, 2000 (IT Act): Section 66D of the IT Act criminalises the use of computer resources for cheating by personation. Section 66E prescribes the punishment for capturing and publishing or transmitting the image of a private area of any person without his or her consent. Publishing or transmitting obscene material for instance, which could be generated by using deepfake technology is a punishable offence under Section 67A and 67B of the IT Act. ● Indian Penal Code (IPC): In addition to the IT Act, certain harms/cybercrimes perpetuated by AI could also fall under the IPC. For instance, identity theft and cheating by personation are offences under Section 419 (cheating by personation), section 463 and 465 (forgery for the purpose of cheating), section 292 and 294 (selling/circulating/distributing obscene objects), and section 499 (causing harm to reputation/defamation). It is to be noted that the IPC has been recently replaced by the Bharatiya Nyaya (Second) Sanhita (BNS2), and the BNS2 retains these offences. ● Other laws: In addition to the IT Act and IPC / BNS2, there could be more laws depending on the nature of crime or cause of actions involved, like Prevention of Children from Sexual Offences Act, 2012 (section 12) in the event of sexual harassment of children, Juvenile Justice (Care and Protection of Children) Act, 2015 (section 75) for causing harm to the children, the Copyrights Act (section 51), if synthetic content infringes copyrighted work. It is worth noting that existing laws can also require specific measures from platforms and online service providers to detect and remove malicious synthetic media. Under the Information Technology (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021: ● Rule 3(1)(b) requires intermediaries to inform its rules and regulations, privacy policy and user agreement to a user and to make reasonable efforts to prevent the dissemination of specific types of content, that may cause harm to its users – including information that may violate bodily privacy, cause harm to a child, is deceptive, among other things. ● Rule 3(1)(c) requires intermediaries to periodically inform their users about the effects of non-compliance with the rules and regulations, privacy policy, or user agreement of such intermediary. ● Rule 3(2)(b) requires the intermediary to, within 24 hours of receiving user complaint of content which is ‘impersonation in electronic form, including artificially morphed images of such individual’ remove or disable its access. Further, the grievance officer appointed by the intermediary should acknowledge user complaints within 24 hours. The above shows that the legal framework may be adequate for the purposes of detecting, preventing, removing, and prosecuting the creation and distribution of malicious synthetic media. However, for this legal framework to be effective, it needs to be backed by requisite capabilities to enable stakeholders to effectively comply and for the authorities to enforce the legal framework. Page 8 of 22 This area points to possible gaps and opportunities for using technological measures for enabling effective compliance, so that malicious deepfakes are detected in time and/ or are removed before they cause serious harm. For example, as suggested above, traceability may be established by assigning unique and immutable identities to different participants, such as content creators, publishers, social media platforms, etc. These may then be used to watermark inputs to, and outputs from, generative AI tools. These may be used to track and analyse the lifecycle, from creation to use, of a deepfake – and to determine when they have been cre

**Question:** How do the Information Technology Act, 2000, and the Indian Penal Code (or its replacement, BNS2) address cybercrimes related to deepfakes?

**LLM Answer:** The Information Technology Act, 2000 (IT Act) and the Indian Penal Code (IPC), or its replacement Bharatiya Nyaya (Second) Sanhita (BNS2), address cybercrimes related to deepfakes through a combination of specific sections tailored for digital crimes and more general provisions found in traditional criminal laws.

1. **Information Technology Act, 2000 (IT Act):**
   - **Section 66D** criminalizes the use of computer resources for cheating by personation.
   - **Section 66E** deals with the capture, storage, transmission, publication, or distribution of information about the private area of any person without their consent. This can apply to deepfakes involving private body parts.
   - **Sections 67A and 67B** criminalize the publishing or transmitting of obscene material in electronic form, which could include deepfake content that is maliciously generated.

2. **Indian Penal Code (IPC) / Bharatiya Nyaya (Second) Sanhita (BNS2):**
   - The IPC retains sections such as:
     - **Section 419** for cheating by personation.
     - **Sections 463 and 465** for forgery for the purpose of cheating, which could apply if deepfake content is used to deceive or defraud someone.
     - **Sections 292 and 294** cover selling, circulating, or distributing obscene objects, which can encompass malicious deepfakes.
     - **Section 499** deals with causing harm to reputation/defamation, relevant if a deepfake is used to damage someone's reputation.

These laws provide a legal framework to address the misuse of synthetic media. However, they need to be enforced effectively by intermediaries and authorities, as supported by technological measures like traceability and watermarking, which can help in detecting and removing malicious deepfakes before serious harm occurs.

In summary, while specific sections of the IT Act and IPC/BNS2 cover various aspects of cybercrimes involving deepfakes, their effectiveness depends on proper implementation and enforcement.

**Human Answer:** The Information Technology Act, 2000 (IT Act), addresses cybercrimes related to deepfakes through several sections:• Section 66D criminalizes the use of computer resources for cheating by personation. This could apply if a deepfake is used to impersonate someone for fraudulent purposes.• Section 66E prescribes punishment for capturing and publishing or transmitting the image of a private area of any person without their consent. Deepfakes creating such images could fall under this section.• Sections 67A and 67B make publishing or transmitting obscene material punishable offences1. This could include obscene content generated using deepfake technology. The Indian Penal Code (IPC) and its recent replacement, the Bharatiya Nyaya (Second) Sanhita (BNS2), also address harms and cybercrimes that could be perpetrated by AI-generated malicious synthetic media: • Section 419 (cheating by personation) of the IPC criminalizes identity theft and cheating by personation1. This offense is retained in the BNS2. •Sections 463 and 465 (forgery for the purpose of cheating) of the IPC address forgery with the intent to deceive1. These offences are also retained in the BNS2. • Sections 292 and 294 (selling/circulating/distributing obscene objects) of the IPC criminalize the dissemination of obscene material. These offences are retained in the BNS2. • Section 499 (causing harm to reputation/defamation) of the IPC addresses defamation1. This offence is also retained in the BNS2. •In addition to these, other laws like the Prevention of Children from Sexual Offences Act, 2012 (section 12), can apply in cases of sexual harassment of children, and the Juvenile Justice (Care and Protection of Children) Act, 2015 (section 75), for causing harm to children1. The Copyrights Act (section 51) may also be relevant if synthetic content infringes copyrighted work

**Scores:** Similarity: 0.938, ROUGE-L: 0.383, BLEU: 0.226, METEOR: 0.374, BERT Precision: 0.863, BERT Recall: 0.878, BERT F1: 0.871

**Question:** What obligations do online intermediaries have under the IT (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021, in detecting and removing malicious synthetic media?

**LLM Answer:** Under the Information Technology (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021, online intermediaries have several obligations in detecting and removing malicious synthetic media:

1. **Prevent Dissemination of Harmful Content**: Rule 3(1)(b) mandates that intermediaries should inform users about their rules, privacy policy, and user agreement. They must also make reasonable efforts to prevent the dissemination of specific types of content that may cause harm, including information that violates bodily privacy or is deceptive.

2. **User Complaint Handling**: Rule 3(2)(b) requires intermediaries to remove or disable access to content reported as 'impersonation in electronic form, including artificially morphed images,' within 24 hours of receiving a user complaint. The intermediary must also acknowledge the complaint from a designated grievance officer within the same timeframe.

These obligations underscore that intermediaries are expected to take proactive measures and respond promptly when faced with reports of malicious synthetic media, thereby fulfilling their responsibility under the legal framework.

**Human Answer:** Under the Information Technology (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021, online intermediaries have specific obligations in detecting and removing malicious synthetic media: •Rule 3(1)(b) requires intermediaries to inform users of their rules, regulations, privacy policy, and user agreement and to make reasonable efforts to prevent the dissemination of specific types of harmful content. This includes information that may violate bodily privacy, cause harm to a child, or is deceptive. •Rule 3(1)(c) mandates that intermediaries periodically inform their users about the consequences of non-compliance with their rules, regulations, privacy policy, or user agreement. •Rule 3(2)(b) requires intermediaries to remove or disable access to content within 24 hours of receiving a user complaint regarding 'impersonation in electronic form, including artificially morphed images of such individual'. Furthermore, the grievance officer appointed by the intermediary must acknowledge user complaints within 24 hour.

**Scores:** Similarity: 0.942, ROUGE-L: 0.575, BLEU: 0.47, METEOR: 0.665, BERT Precision: 0.927, BERT Recall: 0.942, BERT F1: 0.934

**Question:** Why is technological intervention necessary for the effective enforcement of laws against malicious deepfakes?

**LLM Answer:** Technological intervention is necessary for the effective enforcement of laws against malicious deepfakes because:

1. **Detection and Monitoring**: Traditional methods may struggle to identify and monitor synthetic media in real-time. Advanced technologies can help detect deepfake content more accurately and swiftly.

2. **Traceability and Accountability**: By assigning unique, immutable identities to participants (content creators, publishers, platforms), technological measures enable tracking the lifecycle of a deepfake from creation to distribution. This traceability helps hold individuals or entities accountable for their actions.

3. **Automated Removal**: Rules like those under the Information Technology (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021, require intermediaries to remove content that violates specific criteria within a short timeframe. Automation of such processes ensures rapid response to potential violations.

4. **Compliance Verification**: Technological tools can help verify compliance with existing laws by monitoring user behavior, content uploads, and platform operations continuously.

5. **Efficiency in Law Enforcement**: Authorities need efficient means to enforce legal measures effectively. Technology can provide real-time alerts, evidence collection, and analysis capabilities that enhance the enforcement process.

6. **Prevention of Harm**: By detecting malicious deepfakes early or even preventing their creation, technological interventions can mitigate potential harm before it occurs.

In summary, while existing legal frameworks provide a foundation for combating malicious deepfakes, the implementation of technological measures ensures these laws are effectively enforced and respected in practice.

**Human Answer:** While the existing legal framework may be adequate for detecting, preventing, removing, and prosecuting the creation and distribution of malicious synthetic media, technological intervention is necessary for its effective enforcement. This is because technological measures can enable stakeholders to effectively comply with the law, and authorities to enforce it by detecting malicious deepfakes in time and/or removing them before they cause serious harm.

**Scores:** Similarity: 0.795, ROUGE-L: 0.166, BLEU: 0.037, METEOR: 0.404, BERT Precision: 0.83, BERT Recall: 0.894, BERT F1: 0.861

**Question:** How can traceability and watermarking help in tracking and preventing the spread of harmful AI-generated content?

**LLM Answer:** Traceability and watermarking can significantly aid in tracking and preventing the spread of harmful AI-generated content by providing a means to identify and trace the origin, lifecycle, and dissemination of such content. Here's how:

1. **Unique Identifiers for Participants**: By assigning unique and immutable identifiers to different participants (e.g., content creators, publishers, social media platforms), it becomes easier to track interactions within the AI ecosystem.

2. **Watermarking Content**: Watermarks can be embedded in the generated content at various stages—input or output from generative AI tools. These watermarks serve as digital fingerprints that remain even after the content undergoes processing and distribution.

3. **Lifecycle Tracking**: Once watermarked, the lifecycle of the content can be tracked from creation to consumption. This includes monitoring how the content is shared, modified, and used by different intermediaries and users.

4. **Detection and Removal**: Watermarks can help in rapidly identifying malicious or harmful AI-generated content that violates existing laws (such as those mentioned in the context). When such content is flagged, it can be removed promptly to prevent further spread.

5. **Legal Compliance and Enforcement**: With detailed tracking data, enforcement agencies can more effectively monitor compliance with legal measures like the Information Technology (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021, which require platforms to remove certain types of harmful content within a specified timeframe.

6. **Enhanced Accountability**: Watermarks and traceability mechanisms enhance accountability by providing clear evidence of who created or modified the content at various stages. This can be crucial in legal proceedings where identifying the source is necessary.

In summary, integrating traceability and watermarking into AI systems helps in creating a robust framework for detecting, tracking, and preventing the spread of harmful AI-generated content, thereby supporting effective compliance with existing laws and regulations.

**Human Answer:** Traceability and watermarking are suggested as technological measures that can help in tracking and preventing the spread of harmful AI-generated content1. Establishing unique and immutable identities for participants such as content creators, publishers, and social media platforms, and using these to watermark inputs and outputs of generative AI tools, could allow for tracking and analyzing the lifecycle of a deepfake from creation to use. This can help in determining when harmful content has been created and disseminated.

**Scores:** Similarity: 0.82, ROUGE-L: 0.251, BLEU: 0.065, METEOR: 0.351, BERT Precision: 0.841, BERT Recall: 0.911, BERT F1: 0.875


--------------------------------------------------------------------------------
