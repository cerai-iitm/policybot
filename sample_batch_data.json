[
  {
    "id": "topic_1",
    "context": "The Digital Personal Data Protection Bill, 2023 was introduced in Lok Sabha on August 3, 2023. The Bill seeks to provide for the protection of personal data and the privacy of individuals. Applicability: The Bill applies to the processing of digital personal data within India where such data is: (i) collected online, or (ii) collected offline and is digitised. It will also apply to the processing of personal data outside India if it is for offering goods or services in India. Personal data is defined as any data about an individual who is identifiable by or in relation to such data. Processing has been defined as wholly or partially automated operation or set of operations performed on digital personal data. It includes collection, storage, use, and sharing. Consent: Personal data may be processed only for a lawful purpose after obtaining the consent of the individual. A notice must be given before seeking consent. The notice should contain details about the personal data to be collected and the purpose of processing. Consent may be withdrawn at any point in time. Consent will not be required for 'legitimate uses' including: (i) specified purpose for which data has been provided by an individual voluntarily, (ii) provision of benefit or service by the government, (iii) medical emergency, and (iv) employment. For individuals below 18 years of age, consent will be provided by the parent or the legal guardian. Rights and duties of data principal: An individual, whose data is being processed (data principal), will have the right to: (i) obtain information about processing, (ii) seek correction and erasure of personal data, (iii) nominate another person to exercise rights in the event of death or incapacity, and (iv) grievance redressal. Data principals will have certain duties. They must not: (i) register a false or frivolous complaint, and (ii) furnish any false particulars or impersonate another person in specified cases. Violation of duties will be punishable with a penalty of up to Rs 10,000. Obligations of data fiduciaries: The entity, determining the purpose and means of processing, (data fiduciary), must: (i) make reasonable efforts to ensure the accuracy and completeness of data, (ii) build reasonable security safeguards to prevent a data breach, (iii) inform the Data Protection Board of India and affected persons in the event of a breach, and (iv) erase personal data as soon as the purpose has been met and retention is not necessary for legal purposes (storage limitation). In case of government entities, storage limitation and the right of the data principal to erasure will not apply. Significant data fiduciaries: Certain data fiduciaries may be designated as significant data fiduciaries. Certain factors must be taken into regard such as: (i) volume and sensitivity of personal data processed, (ii) risks to the rights of data principals, (iii) security of the state, and (iv) public order. These entities will have certain additional obligations including: (i) appointing a data protection officer, and (ii) undertaking impact assessment and compliance audit. Exemptions: Rights of the data principal and obligations of data fiduciaries (except data security) will not apply in specified cases. These include: (i) prevention and investigation of offences, and (ii) enforcement of legal rights or claims. The central government may, by notification, exempt certain activities from the application of the Bill. These include: (i) processing by government entities in the interest of the security of the state and public order, and (ii) research, archiving, or statistical purposes. Processing of personal data of children: While processing the personal data of a child, the data fiduciary must not undertake: (i) processing that is likely to cause any detrimental effect on the well-being of the child, and (ii) tracking, behavioural monitoring, or targeted advertising. Cross-border transfer: The Bill allows the transfer of personal data outside India, except to countries restricted by the government through notification. Data Protection Board of India: The central government will establish the Data Protection Board of India. Key functions of the Board include: (i) monitoring compliance and imposing penalties, (ii) directing data fiduciaries to take necessary measures in the event of a data breach, and (iii) hearing grievances made by affected persons. Board members will be appointed for two years and will be eligible for re-appointment. Penalties: The schedule to the Bill specifies penalties for various offences such as up to: (i) Rs 200 crore for non-fulfilment of obligations for children, and (ii) Rs 250 crore for failure to take security measures to prevent data breaches.",
    "qa_pairs": [
      {
        "question": "What are the key applicability criteria for the Digital Personal Data Protection Bill, 2023?",
        "human_answer": "Applicability Criteria: The Bill applies to the processing of digital personal data within India if the data is (i) collected online, or (ii) collected offline and then digitized. It also applies to processing personal data outside India if it involves offering goods or services within India."
      },
      {
        "question": "Under what conditions can personal data be processed without the consent of the individual?",
        "human_answer": "Conditions for Processing Data Without Consent: Personal data can be processed without individual consent for 'legitimate uses,' including when data is voluntarily provided for a specified purpose, for the provision of government benefits or services, during medical emergencies, and for employment purposes."
      },
      {
        "question": "What rights are granted to data principals under the Bill?",
        "human_answer": "Rights of Data Principals: Data principals, or individuals whose data is being processed, have the right to (i) obtain information about the processing of their data, (ii) seek correction and erasure of their personal data, (iii) nominate someone to exercise their rights if they die or become incapacitated, and (iv) grievance redressal."
      },
      {
        "question": "What obligations do data fiduciaries have in ensuring data protection and security?",
        "human_answer": "Obligations of Data Fiduciaries: Data fiduciaries, entities determining the purpose and means of data processing, must (i) make reasonable efforts to ensure data accuracy and completeness, (ii) build reasonable security safeguards to prevent data breaches, (iii) inform the Data Protection Board of India and affected individuals of data breaches, and (iv) erase personal data when its purpose has been met and retention is no longer legally necessary. Note that storage limitation and the right of the data principal to erasure do not apply to government entities."
      },
      {
        "question": "What are the penalties specified in the Bill for non-compliance with data protection obligations?",
        "human_answer": "Penalties for Non-Compliance: The Bill specifies penalties for various offenses, including (i) up to Rs 200 crore for not fulfilling obligations related to children's data, and (ii) up to Rs 250 crore for failing to implement security measures to prevent data breaches"
      }
    ]
  },
  {
    "id": "topic_2",
    "context": "The committee recommends the following: 1. To implement a whole-of-government approach to AI Governance, MeitY and the Principal Scientific Adviser should establish an empowered mechanism to coordinate AI Governance. The empowered mechanism should be in the form of an Inter-Ministerial AI Coordination Committee or Governance Group (Committee/ Group). It should bring together the various authorities and institutions that deal with AI Governance at the national level. The Committee/ Group should have an ongoing status and should not be a limited duration mechanism. The overall purpose of this Committee/ Group should be to bring the key institutions around a common roadmap and to coordinate their efforts to implement a whole-of government approach. A collaborative and co-ordinated approach by various regulators can enable them to be more efficient and effective, given the complexity likely to be involved in dealing with AI systems at scale, especially when we take a long-term view of the diffusion stage of their lifecycle. This can be especially necessary in domains and areas where multiple authorities may be concerned (e.g., consumer protection, food, transportation, agriculture, health care, etc.). The Committee/ Group should enable a whole of government approach to the AI ecosystem. Currently, regulators and government departments may have some visibility on the AI systems developed or deployed by entities who are under sectoral regulation (e.g., finance or health) or where the market is concentrated (e.g., ecommerce, social media, aggregators). However, the level of visibility would need to be adequate to assess potential risks associated with such entities in the context of AI. Further, there are likely to be AI systems developed or being developed and/ or deployed by entities who may not have an interface with the government/ regulators from a perspective of affording suitable visibility to enable a risk assessment in relation to AI. A pre-requisite of governance would be for the government and the regulators to have a credible understanding the AI ecosystem in the country so that governance measures are rooted to the realities of existing and likely risks. The Committee/ Group should facilitate this task. This would require a conversation-led approach with a view to develop an understanding of the ecosystem which can both serve as feedback for strengthening governance and enable understanding of possible challenges and gaps in complying Page 13 of 22 and enforcing existing laws. It is important to emphasise that such a mapping exercise should not result in regulatory overreach through at scale registration and reporting requirements. With the above context, the Committee/ Group should meet at a regular basis to suggest measures to catalyse collaboration between departments and regulators, so that they can: • apply and strengthen existing laws to minimise risk of harm due to use of AI; • provide legal clarity and certainty around development and use of AI by issuing joint guidance; • harmonise existing efforts and initiatives around common terminologies and risk inventories; • enable demonstrable self-regulation to operationalise the responsible AI principles; • take coordinated steps to respond to identified gaps with the benefit of multiregulatory support; • create a policy environment which enables the use of AI for beneficial use-cases; and • promote the development and deployment of responsible AI applications in their domains/sectors. In order to enable appropriate measurement of fairness, accountability and transparency in the Indian context, it is an essential prerequisite to have access to the right datasets, relevant to the Indian context, which allows users to assess the fairness and bias of their models across standard datasets. The creation of better datasets for the Indian context should be stimulated, and sector-specific datasets should be identified to enable creation and evaluation of fair models. These initiatives may be encouraged by the Committee/ Group. The Committee/ Group should have a mix of both official and non-official members, because such a forum focused on coordinating AI governance must also bring in external expertise from industry and academia, given their central role in operationalising responsible AI principles in practice. It may be headed by the Principal Scientific Adviser. Official members could include representatives deputed from MeitY, the NITI Aayog, the Telecommunication Engineering Centre, Bureau of Indian Standards, other departments of the Central Government, as well as sectoral regulators (e.g., RBI, Indian Council of Medical Research, SEBI, IRDAI, Telecom Regulatory Authority of India, etc.). Non-official members could include persons capable of representing the interests of AI developers, AI deployers, data providers, data principals, and end-users – so that the perspectives of the overall ecosystem can be considered. The Committee/ Group should invite external experts for discussions to understand and take on board diverse perspectives.",
    "qa_pairs": [
      {
        "question": "What is the primary objective of the proposed Inter-Ministerial AI Coordination Committee or Governance Group?",
        "human_answer": "Primary Objective: The primary objective of the proposed Inter-Ministerial AI Coordination Committee or Governance Group is to bring key institutions together around a common roadmap and to coordinate their efforts to implement a whole-of-government approach to AI governance1. This includes enabling a whole-of-government approach to the AI ecosystem."
      },
      {
        "question": "Why is a whole-of-government approach considered essential for AI governance in India?",
        "human_answer": "Importance of a Whole-of-Government Approach: A whole-of-government approach is considered essential because a collaborative and coordinated approach by various regulators can enable them to be more efficient and effective, given the complexity likely to be involved in dealing with AI systems at scale, especially in the long term1. This is particularly necessary in areas where multiple authorities may be concerned, such as consumer protection, food, transportation, agriculture, and health care."
      },
      {
        "question": "What are the key functions and responsibilities of the AI Governance Committee/Group?",
        "human_answer": "Key Functions and Responsibilities: The AI Governance Committee/Group is responsible for: 1) Suggesting measures to catalyze collaboration between departments and regulators. 2) Applying and strengthening existing laws to minimize the risk of harm due to the use of AI. 3) Providing legal clarity and certainty around the development and use of AI by issuing joint guidance.4) Harmonizing existing efforts and initiatives around common terminologies and risk inventories. 5) Enabling demonstrable self-regulation to operationalize responsible AI principles. 6) Taking coordinated steps to respond to identified gaps with multi-regulatory support. 7) Creating a policy environment that enables the use of AI for beneficial use-cases. 8) Promoting the development and deployment of responsible AI applications in various domains and sectors."
      },
      {
        "question": "How does the committee propose to ensure fairness, accountability, and transparency in AI systems within the Indian context?",
        "human_answer": "Ensuring Fairness, Accountability, and Transparency: The committee proposes to ensure fairness, accountability, and transparency in AI systems by: ◦ Promoting the creation of better datasets relevant to the Indian context, which allows users to assess the fairness and bias of their models across standard datasets. ◦ Identifying sector-specific datasets to enable the creation and evaluation of fair models."
      },
      {
        "question": "Who are the proposed members of the AI Governance Committee/Group, and why is it important to include both official and non-official members?",
        "human_answer": "Proposed Members: The AI Governance Committee/Group should have a mix of both official and non-official members. ◦Official members could include representatives from MeitY, NITI Aayog, the Telecommunication Engineering Centre, Bureau of Indian Standards, other central government departments, and sectoral regulators such as RBI, Indian Council of Medical Research, SEBI, IRDAI, and Telecom Regulatory Authority of India. ◦Non-official members could include individuals representing the interests of AI developers, AI deployers, data providers, data principals, and end-users. ◦It is important to include both official and non-official members to bring in external expertise from industry and academia, given their central role in operationalizing responsible AI principles in practice, and to consider the perspectives of the overall ecosystem1. The Committee/ Group should invite external experts for discussions to understand and take on board diverse perspectives."
      }
    ]
  },
  {
    "id": "topic_3",
    "context": "The need to enable effective compliance and enforcement of existing laws. 1. Deepfakes/ fakes/ malicious content There are existing legal safeguards/instruments to protect against misuse of foundation models for creating malicious synthetic media (i.e., malicious ‘deepfakes’). In this case, depending upon the context and negative effect of the malicious synthetic media in question, multiple laws can apply. For example: See: Rahul Matthan, The Zone of Mischief (2024) 8 Page 7 of 22 ● Information Technology Act, 2000 (IT Act): Section 66D of the IT Act criminalises the use of computer resources for cheating by personation. Section 66E prescribes the punishment for capturing and publishing or transmitting the image of a private area of any person without his or her consent. Publishing or transmitting obscene material for instance, which could be generated by using deepfake technology is a punishable offence under Section 67A and 67B of the IT Act. ● Indian Penal Code (IPC): In addition to the IT Act, certain harms/cybercrimes perpetuated by AI could also fall under the IPC. For instance, identity theft and cheating by personation are offences under Section 419 (cheating by personation), section 463 and 465 (forgery for the purpose of cheating), section 292 and 294 (selling/circulating/distributing obscene objects), and section 499 (causing harm to reputation/defamation). It is to be noted that the IPC has been recently replaced by the Bharatiya Nyaya (Second) Sanhita (BNS2), and the BNS2 retains these offences. ● Other laws: In addition to the IT Act and IPC / BNS2, there could be more laws depending on the nature of crime or cause of actions involved, like Prevention of Children from Sexual Offences Act, 2012 (section 12) in the event of sexual harassment of children, Juvenile Justice (Care and Protection of Children) Act, 2015 (section 75) for causing harm to the children, the Copyrights Act (section 51), if synthetic content infringes copyrighted work. It is worth noting that existing laws can also require specific measures from platforms and online service providers to detect and remove malicious synthetic media. Under the Information Technology (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021: ● Rule 3(1)(b) requires intermediaries to inform its rules and regulations, privacy policy and user agreement to a user and to make reasonable efforts to prevent the dissemination of specific types of content, that may cause harm to its users – including information that may violate bodily privacy, cause harm to a child, is deceptive, among other things. ● Rule 3(1)(c) requires intermediaries to periodically inform their users about the effects of non-compliance with the rules and regulations, privacy policy, or user agreement of such intermediary. ● Rule 3(2)(b) requires the intermediary to, within 24 hours of receiving user complaint of content which is ‘impersonation in electronic form, including artificially morphed images of such individual’ remove or disable its access. Further, the grievance officer appointed by the intermediary should acknowledge user complaints within 24 hours. The above shows that the legal framework may be adequate for the purposes of detecting, preventing, removing, and prosecuting the creation and distribution of malicious synthetic media. However, for this legal framework to be effective, it needs to be backed by requisite capabilities to enable stakeholders to effectively comply and for the authorities to enforce the legal framework. Page 8 of 22 This area points to possible gaps and opportunities for using technological measures for enabling effective compliance, so that malicious deepfakes are detected in time and/ or are removed before they cause serious harm. For example, as suggested above, traceability may be established by assigning unique and immutable identities to different participants, such as content creators, publishers, social media platforms, etc. These may then be used to watermark inputs to, and outputs from, generative AI tools. These may be used to track and analyse the lifecycle, from creation to use, of a deepfake – and to determine when they have been cre",
    "qa_pairs": [
      {
        "question": "How do the Information Technology Act, 2000, and the Indian Penal Code (or its replacement, BNS2) address cybercrimes related to deepfakes?",
        "human_answer": "The Information Technology Act, 2000 (IT Act), addresses cybercrimes related to deepfakes through several sections:• Section 66D criminalizes the use of computer resources for cheating by personation. This could apply if a deepfake is used to impersonate someone for fraudulent purposes.• Section 66E prescribes punishment for capturing and publishing or transmitting the image of a private area of any person without their consent. Deepfakes creating such images could fall under this section.• Sections 67A and 67B make publishing or transmitting obscene material punishable offences1. This could include obscene content generated using deepfake technology. The Indian Penal Code (IPC) and its recent replacement, the Bharatiya Nyaya (Second) Sanhita (BNS2), also address harms and cybercrimes that could be perpetrated by AI-generated malicious synthetic media: • Section 419 (cheating by personation) of the IPC criminalizes identity theft and cheating by personation1. This offense is retained in the BNS2. •Sections 463 and 465 (forgery for the purpose of cheating) of the IPC address forgery with the intent to deceive1. These offences are also retained in the BNS2. • Sections 292 and 294 (selling/circulating/distributing obscene objects) of the IPC criminalize the dissemination of obscene material. These offences are retained in the BNS2. • Section 499 (causing harm to reputation/defamation) of the IPC addresses defamation1. This offence is also retained in the BNS2. •In addition to these, other laws like the Prevention of Children from Sexual Offences Act, 2012 (section 12), can apply in cases of sexual harassment of children, and the Juvenile Justice (Care and Protection of Children) Act, 2015 (section 75), for causing harm to children1. The Copyrights Act (section 51) may also be relevant if synthetic content infringes copyrighted work"
      },
      {
        "question": "What obligations do online intermediaries have under the IT (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021, in detecting and removing malicious synthetic media?",
        "human_answer": "Under the Information Technology (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021, online intermediaries have specific obligations in detecting and removing malicious synthetic media: •Rule 3(1)(b) requires intermediaries to inform users of their rules, regulations, privacy policy, and user agreement and to make reasonable efforts to prevent the dissemination of specific types of harmful content. This includes information that may violate bodily privacy, cause harm to a child, or is deceptive. •Rule 3(1)(c) mandates that intermediaries periodically inform their users about the consequences of non-compliance with their rules, regulations, privacy policy, or user agreement. •Rule 3(2)(b) requires intermediaries to remove or disable access to content within 24 hours of receiving a user complaint regarding 'impersonation in electronic form, including artificially morphed images of such individual'. Furthermore, the grievance officer appointed by the intermediary must acknowledge user complaints within 24 hour."
      },
      {
        "question": "Why is technological intervention necessary for the effective enforcement of laws against malicious deepfakes?",
        "human_answer": "While the existing legal framework may be adequate for detecting, preventing, removing, and prosecuting the creation and distribution of malicious synthetic media, technological intervention is necessary for its effective enforcement. This is because technological measures can enable stakeholders to effectively comply with the law, and authorities to enforce it by detecting malicious deepfakes in time and/or removing them before they cause serious harm."
      },
      {
        "question": "How can traceability and watermarking help in tracking and preventing the spread of harmful AI-generated content?",
        "human_answer": "Traceability and watermarking are suggested as technological measures that can help in tracking and preventing the spread of harmful AI-generated content1. Establishing unique and immutable identities for participants such as content creators, publishers, and social media platforms, and using these to watermark inputs and outputs of generative AI tools, could allow for tracking and analyzing the lifecycle of a deepfake from creation to use. This can help in determining when harmful content has been created and disseminated."
      }
    ]
  }
]
